{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Humman Activity Recognization using LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "5ZlWTRAADhbZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# HumanActivityRecognition\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "This project is to build a model that predicts the human activities such as Walking, Walking_Upstairs, Walking_Downstairs, Sitting, Standing or Laying.\n",
        "\n",
        "This dataset is collected from 30 persons(referred as subjects in this dataset), performing different activities with a smartphone to their waists. The data is recorded with the help of sensors (accelerometer and Gyroscope) in that smartphone. This experiment was video recorded to label the data manually.\n",
        "\n",
        "## How data was recorded\n",
        "\n",
        "By using the sensors(Gyroscope and accelerometer) in a smartphone, they have captured '3-axial linear acceleration'(_tAcc-XYZ_) from accelerometer and '3-axial angular velocity' (_tGyro-XYZ_) from Gyroscope with several variations. \n",
        "\n",
        "> prefix 't' in those metrics denotes time.\n",
        "\n",
        "> suffix 'XYZ' represents 3-axial signals in X , Y, and Z directions.\n",
        "\n",
        "### Feature names\n",
        "\n",
        "1. These sensor signals are preprocessed by applying noise filters and then sampled in fixed-width windows(sliding windows) of 2.56 seconds each with 50% overlap. ie., each window has 128 readings. \n",
        "\n",
        "2. From Each window, a feature vector was obtianed by calculating variables from the time and frequency domain.\n",
        "> In our dataset, each datapoint represents a window with different readings \n",
        "3. The accelertion signal was saperated into Body and Gravity acceleration signals(___tBodyAcc-XYZ___ and ___tGravityAcc-XYZ___) using some low pass filter with corner frequecy of 0.3Hz.\n",
        "\n",
        "4. After that, the body linear acceleration and angular velocity were derived in time to obtian _jerk signals_ (___tBodyAccJerk-XYZ___ and ___tBodyGyroJerk-XYZ___). \n",
        "\n",
        "5. The magnitude of these 3-dimensional signals were calculated using the Euclidian norm. This magnitudes are represented as features with names like _tBodyAccMag_, _tGravityAccMag_, _tBodyAccJerkMag_, _tBodyGyroMag_ and _tBodyGyroJerkMag_.\n",
        "\n",
        "6. Finally, We've got frequency domain signals from some of the available signals by applying a FFT (Fast Fourier Transform). These signals obtained were labeled with ___prefix 'f'___ just like original signals with ___prefix 't'___. These signals are labeled as ___fBodyAcc-XYZ___, ___fBodyGyroMag___ etc.,.\n",
        "\n",
        "7. These are the signals that we got so far.\n",
        "\t+ tBodyAcc-XYZ\n",
        "\t+ tGravityAcc-XYZ\n",
        "\t+ tBodyAccJerk-XYZ\n",
        "\t+ tBodyGyro-XYZ\n",
        "\t+ tBodyGyroJerk-XYZ\n",
        "\t+ tBodyAccMag\n",
        "\t+ tGravityAccMag\n",
        "\t+ tBodyAccJerkMag\n",
        "\t+ tBodyGyroMag\n",
        "\t+ tBodyGyroJerkMag\n",
        "\t+ fBodyAcc-XYZ\n",
        "\t+ fBodyAccJerk-XYZ\n",
        "\t+ fBodyGyro-XYZ\n",
        "\t+ fBodyAccMag\n",
        "\t+ fBodyAccJerkMag\n",
        "\t+ fBodyGyroMag\n",
        "\t+ fBodyGyroJerkMag\n",
        "\n",
        "8. We can esitmate some set of variables from the above signals. ie., We will estimate the following properties on each and every signal that we recoreded so far.\n",
        "\n",
        "\t+ ___mean()___: Mean value\n",
        "\t+ ___std()___: Standard deviation\n",
        "\t+ ___mad()___: Median absolute deviation \n",
        "\t+ ___max()___: Largest value in array\n",
        "\t+ ___min()___: Smallest value in array\n",
        "\t+ ___sma()___: Signal magnitude area\n",
        "\t+ ___energy()___: Energy measure. Sum of the squares divided by the number of values. \n",
        "\t+ ___iqr()___: Interquartile range \n",
        "\t+ ___entropy()___: Signal entropy\n",
        "\t+ ___arCoeff()___: Autorregresion coefficients with Burg order equal to 4\n",
        "\t+ ___correlation()___: correlation coefficient between two signals\n",
        "\t+ ___maxInds()___: index of the frequency component with largest magnitude\n",
        "\t+ ___meanFreq()___: Weighted average of the frequency components to obtain a mean frequency\n",
        "\t+ ___skewness()___: skewness of the frequency domain signal \n",
        "\t+ ___kurtosis()___: kurtosis of the frequency domain signal \n",
        "\t+ ___bandsEnergy()___: Energy of a frequency interval within the 64 bins of the FFT of each window.\n",
        "\t+ ___angle()___: Angle between to vectors.\n",
        "\n",
        "9. We can obtain some other vectors by taking the average of signals in a single window sample. These are used on the angle() variable'\n",
        "`\n",
        "\t+ gravityMean\n",
        "\t+ tBodyAccMean\n",
        "\t+ tBodyAccJerkMean\n",
        "\t+ tBodyGyroMean\n",
        "\t+ tBodyGyroJerkMean\n",
        "\n",
        "\n",
        "###  Y_Labels(Encoded)\n",
        "+ In the dataset, Y_labels are represented as numbers from 1 to 6 as their identifiers.\n",
        "\n",
        "\t- WALKING as __1__\n",
        "\t- WALKING_UPSTAIRS as __2__\n",
        "\t- WALKING_DOWNSTAIRS as __3__\n",
        "\t- SITTING as __4__\n",
        "\t- STANDING as __5__\n",
        "\t- LAYING as __6__\n",
        "    \n",
        "## Train and test data were saperated\n",
        " - The readings from ___70%___ of the volunteers were taken as ___trianing data___ and remaining ___30%___ subjects recordings were taken for ___test data___\n",
        " \n",
        "## Data\n",
        "\n",
        "* All the data is present in 'UCI_HAR_dataset/' folder in present working directory.\n",
        "     - Feature names are present in 'UCI_HAR_dataset/features.txt'\n",
        "     - ___Train Data___\n",
        "         - 'UCI_HAR_dataset/train/X_train.txt'\n",
        "         - 'UCI_HAR_dataset/train/subject_train.txt'\n",
        "         - 'UCI_HAR_dataset/train/y_train.txt'\n",
        "     - ___Test Data___\n",
        "         - 'UCI_HAR_dataset/test/X_test.txt'\n",
        "         - 'UCI_HAR_dataset/test/subject_test.txt'\n",
        "         - 'UCI_HAR_dataset/test/y_test.txt'\n",
        "         \n",
        "\n",
        "## Data Size :\n",
        "> 27 MB\n"
      ]
    },
    {
      "metadata": {
        "id": "mA6gxpYFEKKr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Quick overview of the dataset :"
      ]
    },
    {
      "metadata": {
        "id": "5-igF2AcEE83",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Accelerometer and Gyroscope readings are taken from 30 volunteers(referred as subjects) while performing the following 6 Activities.\n",
        "\n",
        "    1. Walking     \n",
        "    2. WalkingUpstairs \n",
        "    3. WalkingDownstairs \n",
        "    4. Standing \n",
        "    5. Sitting \n",
        "    6. Lying.\n",
        "\n",
        "\n",
        "* Readings are divided into a window of 2.56 seconds with 50% overlapping. \n",
        "\n",
        "* Accelerometer readings are divided into gravity acceleration and body acceleration readings,\n",
        "  which has x,y and z components each.\n",
        "\n",
        "* Gyroscope readings are the measure of angular velocities which has x,y and z components.\n",
        "\n",
        "* Jerk signals are calculated for BodyAcceleration readings.\n",
        "\n",
        "* Fourier Transforms are made on the above time readings to obtain frequency readings.\n",
        "\n",
        "* Now, on all the base signal readings., mean, max, mad, sma, arcoefficient, engerybands,entropy etc., are calculated for each window.\n",
        "\n",
        "* We get a feature vector of 561 features and these features are given in the dataset.\n",
        "\n",
        "* Each window of readings is a datapoint of 561 features.\n",
        "\n",
        "## Problem Framework\n",
        "\n",
        "* 30 subjects(volunteers) data is randomly split to 70%(21) test and 30%(7) train data.\n",
        "* Each datapoint corresponds one of the 6 Activities.\n"
      ]
    },
    {
      "metadata": {
        "id": "W5REhAU_EO-v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Problem Statement\n",
        "Given a new datapoint we have to predict the Activity"
      ]
    },
    {
      "metadata": {
        "id": "5y7qP87rD0nz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Importing Libraries"
      ]
    },
    {
      "metadata": {
        "id": "0aEZESZl5L5Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7K3JEKtlu3LL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Obtain the train and test data"
      ]
    },
    {
      "metadata": {
        "id": "PerAzwgdv1hC",
        "colab_type": "code",
        "outputId": "da08101c-62b4-4344-d1f5-f2c55bc2dd15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "C_txgntCvbdU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp  -r \"/content/drive/My Drive/UCI_HAR_Dataset/csv_files/train.csv\" \"train.csv\"\n",
        "!cp  -r \"/content/drive/My Drive/UCI_HAR_Dataset/csv_files/test.csv\" \"test.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6SBuaVChu-zY",
        "colab_type": "code",
        "outputId": "dd3ce0ed-ca8c-4b9b-b642-44b0d7b81d57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "print(train.shape, test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7352, 564) (2947, 564)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZiiyZ-njvKAE",
        "colab_type": "code",
        "outputId": "70533098-6527-4742-930f-876321c069de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "cell_type": "code",
      "source": [
        "train.head(3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tBodyAccmeanX</th>\n",
              "      <th>tBodyAccmeanY</th>\n",
              "      <th>tBodyAccmeanZ</th>\n",
              "      <th>tBodyAccstdX</th>\n",
              "      <th>tBodyAccstdY</th>\n",
              "      <th>tBodyAccstdZ</th>\n",
              "      <th>tBodyAccmadX</th>\n",
              "      <th>tBodyAccmadY</th>\n",
              "      <th>tBodyAccmadZ</th>\n",
              "      <th>tBodyAccmaxX</th>\n",
              "      <th>...</th>\n",
              "      <th>angletBodyAccMeangravity</th>\n",
              "      <th>angletBodyAccJerkMeangravityMean</th>\n",
              "      <th>angletBodyGyroMeangravityMean</th>\n",
              "      <th>angletBodyGyroJerkMeangravityMean</th>\n",
              "      <th>angleXgravityMean</th>\n",
              "      <th>angleYgravityMean</th>\n",
              "      <th>angleZgravityMean</th>\n",
              "      <th>subject</th>\n",
              "      <th>Activity</th>\n",
              "      <th>ActivityName</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.288585</td>\n",
              "      <td>-0.020294</td>\n",
              "      <td>-0.132905</td>\n",
              "      <td>-0.995279</td>\n",
              "      <td>-0.983111</td>\n",
              "      <td>-0.913526</td>\n",
              "      <td>-0.995112</td>\n",
              "      <td>-0.983185</td>\n",
              "      <td>-0.923527</td>\n",
              "      <td>-0.934724</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.112754</td>\n",
              "      <td>0.030400</td>\n",
              "      <td>-0.464761</td>\n",
              "      <td>-0.018446</td>\n",
              "      <td>-0.841247</td>\n",
              "      <td>0.179941</td>\n",
              "      <td>-0.058627</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>STANDING</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.278419</td>\n",
              "      <td>-0.016411</td>\n",
              "      <td>-0.123520</td>\n",
              "      <td>-0.998245</td>\n",
              "      <td>-0.975300</td>\n",
              "      <td>-0.960322</td>\n",
              "      <td>-0.998807</td>\n",
              "      <td>-0.974914</td>\n",
              "      <td>-0.957686</td>\n",
              "      <td>-0.943068</td>\n",
              "      <td>...</td>\n",
              "      <td>0.053477</td>\n",
              "      <td>-0.007435</td>\n",
              "      <td>-0.732626</td>\n",
              "      <td>0.703511</td>\n",
              "      <td>-0.844788</td>\n",
              "      <td>0.180289</td>\n",
              "      <td>-0.054317</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>STANDING</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.279653</td>\n",
              "      <td>-0.019467</td>\n",
              "      <td>-0.113462</td>\n",
              "      <td>-0.995380</td>\n",
              "      <td>-0.967187</td>\n",
              "      <td>-0.978944</td>\n",
              "      <td>-0.996520</td>\n",
              "      <td>-0.963668</td>\n",
              "      <td>-0.977469</td>\n",
              "      <td>-0.938692</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.118559</td>\n",
              "      <td>0.177899</td>\n",
              "      <td>0.100699</td>\n",
              "      <td>0.808529</td>\n",
              "      <td>-0.848933</td>\n",
              "      <td>0.180637</td>\n",
              "      <td>-0.049118</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>STANDING</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 564 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   tBodyAccmeanX  tBodyAccmeanY  tBodyAccmeanZ  tBodyAccstdX  tBodyAccstdY  \\\n",
              "0       0.288585      -0.020294      -0.132905     -0.995279     -0.983111   \n",
              "1       0.278419      -0.016411      -0.123520     -0.998245     -0.975300   \n",
              "2       0.279653      -0.019467      -0.113462     -0.995380     -0.967187   \n",
              "\n",
              "   tBodyAccstdZ  tBodyAccmadX  tBodyAccmadY  tBodyAccmadZ  tBodyAccmaxX  \\\n",
              "0     -0.913526     -0.995112     -0.983185     -0.923527     -0.934724   \n",
              "1     -0.960322     -0.998807     -0.974914     -0.957686     -0.943068   \n",
              "2     -0.978944     -0.996520     -0.963668     -0.977469     -0.938692   \n",
              "\n",
              "       ...       angletBodyAccMeangravity  angletBodyAccJerkMeangravityMean  \\\n",
              "0      ...                      -0.112754                          0.030400   \n",
              "1      ...                       0.053477                         -0.007435   \n",
              "2      ...                      -0.118559                          0.177899   \n",
              "\n",
              "   angletBodyGyroMeangravityMean  angletBodyGyroJerkMeangravityMean  \\\n",
              "0                      -0.464761                          -0.018446   \n",
              "1                      -0.732626                           0.703511   \n",
              "2                       0.100699                           0.808529   \n",
              "\n",
              "   angleXgravityMean  angleYgravityMean  angleZgravityMean  subject  Activity  \\\n",
              "0          -0.841247           0.179941          -0.058627        1         5   \n",
              "1          -0.844788           0.180289          -0.054317        1         5   \n",
              "2          -0.848933           0.180637          -0.049118        1         5   \n",
              "\n",
              "   ActivityName  \n",
              "0      STANDING  \n",
              "1      STANDING  \n",
              "2      STANDING  \n",
              "\n",
              "[3 rows x 564 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "WSWRAvPkvLOc",
        "colab_type": "code",
        "outputId": "16ab5816-34fa-4d36-bc0a-2466047fc795",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "cell_type": "code",
      "source": [
        "test.head(3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tBodyAccmeanX</th>\n",
              "      <th>tBodyAccmeanY</th>\n",
              "      <th>tBodyAccmeanZ</th>\n",
              "      <th>tBodyAccstdX</th>\n",
              "      <th>tBodyAccstdY</th>\n",
              "      <th>tBodyAccstdZ</th>\n",
              "      <th>tBodyAccmadX</th>\n",
              "      <th>tBodyAccmadY</th>\n",
              "      <th>tBodyAccmadZ</th>\n",
              "      <th>tBodyAccmaxX</th>\n",
              "      <th>...</th>\n",
              "      <th>angletBodyAccMeangravity</th>\n",
              "      <th>angletBodyAccJerkMeangravityMean</th>\n",
              "      <th>angletBodyGyroMeangravityMean</th>\n",
              "      <th>angletBodyGyroJerkMeangravityMean</th>\n",
              "      <th>angleXgravityMean</th>\n",
              "      <th>angleYgravityMean</th>\n",
              "      <th>angleZgravityMean</th>\n",
              "      <th>subject</th>\n",
              "      <th>Activity</th>\n",
              "      <th>ActivityName</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.257178</td>\n",
              "      <td>-0.023285</td>\n",
              "      <td>-0.014654</td>\n",
              "      <td>-0.938404</td>\n",
              "      <td>-0.920091</td>\n",
              "      <td>-0.667683</td>\n",
              "      <td>-0.952501</td>\n",
              "      <td>-0.925249</td>\n",
              "      <td>-0.674302</td>\n",
              "      <td>-0.894088</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006462</td>\n",
              "      <td>0.162920</td>\n",
              "      <td>-0.825886</td>\n",
              "      <td>0.271151</td>\n",
              "      <td>-0.720009</td>\n",
              "      <td>0.276801</td>\n",
              "      <td>-0.057978</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>STANDING</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.286027</td>\n",
              "      <td>-0.013163</td>\n",
              "      <td>-0.119083</td>\n",
              "      <td>-0.975415</td>\n",
              "      <td>-0.967458</td>\n",
              "      <td>-0.944958</td>\n",
              "      <td>-0.986799</td>\n",
              "      <td>-0.968401</td>\n",
              "      <td>-0.945823</td>\n",
              "      <td>-0.894088</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.083495</td>\n",
              "      <td>0.017500</td>\n",
              "      <td>-0.434375</td>\n",
              "      <td>0.920593</td>\n",
              "      <td>-0.698091</td>\n",
              "      <td>0.281343</td>\n",
              "      <td>-0.083898</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>STANDING</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.275485</td>\n",
              "      <td>-0.026050</td>\n",
              "      <td>-0.118152</td>\n",
              "      <td>-0.993819</td>\n",
              "      <td>-0.969926</td>\n",
              "      <td>-0.962748</td>\n",
              "      <td>-0.994403</td>\n",
              "      <td>-0.970735</td>\n",
              "      <td>-0.963483</td>\n",
              "      <td>-0.939260</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.034956</td>\n",
              "      <td>0.202302</td>\n",
              "      <td>0.064103</td>\n",
              "      <td>0.145068</td>\n",
              "      <td>-0.702771</td>\n",
              "      <td>0.280083</td>\n",
              "      <td>-0.079346</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>STANDING</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 564 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   tBodyAccmeanX  tBodyAccmeanY  tBodyAccmeanZ  tBodyAccstdX  tBodyAccstdY  \\\n",
              "0       0.257178      -0.023285      -0.014654     -0.938404     -0.920091   \n",
              "1       0.286027      -0.013163      -0.119083     -0.975415     -0.967458   \n",
              "2       0.275485      -0.026050      -0.118152     -0.993819     -0.969926   \n",
              "\n",
              "   tBodyAccstdZ  tBodyAccmadX  tBodyAccmadY  tBodyAccmadZ  tBodyAccmaxX  \\\n",
              "0     -0.667683     -0.952501     -0.925249     -0.674302     -0.894088   \n",
              "1     -0.944958     -0.986799     -0.968401     -0.945823     -0.894088   \n",
              "2     -0.962748     -0.994403     -0.970735     -0.963483     -0.939260   \n",
              "\n",
              "       ...       angletBodyAccMeangravity  angletBodyAccJerkMeangravityMean  \\\n",
              "0      ...                       0.006462                          0.162920   \n",
              "1      ...                      -0.083495                          0.017500   \n",
              "2      ...                      -0.034956                          0.202302   \n",
              "\n",
              "   angletBodyGyroMeangravityMean  angletBodyGyroJerkMeangravityMean  \\\n",
              "0                      -0.825886                           0.271151   \n",
              "1                      -0.434375                           0.920593   \n",
              "2                       0.064103                           0.145068   \n",
              "\n",
              "   angleXgravityMean  angleYgravityMean  angleZgravityMean  subject  Activity  \\\n",
              "0          -0.720009           0.276801          -0.057978        2         5   \n",
              "1          -0.698091           0.281343          -0.083898        2         5   \n",
              "2          -0.702771           0.280083          -0.079346        2         5   \n",
              "\n",
              "   ActivityName  \n",
              "0      STANDING  \n",
              "1      STANDING  \n",
              "2      STANDING  \n",
              "\n",
              "[3 rows x 564 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "wZPGW2FyvQwi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# get X_train and y_train from csv files\n",
        "X_train = train.drop(['subject', 'Activity', 'ActivityName'], axis=1)\n",
        "y_train = train.ActivityName"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9DeHSIRCvRUf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# get X_test and y_test from test csv file\n",
        "X_test = test.drop(['subject', 'Activity', 'ActivityName'], axis=1)\n",
        "y_test = test.ActivityName"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ns-yM9fqvTWE",
        "colab_type": "code",
        "outputId": "d4e9f2a0-7054-4f44-9324-9d5cc10a6344",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print('X_train and y_train : ({},{})'.format(X_train.shape, y_train.shape))\n",
        "print('X_test  and y_test  : ({},{})'.format(X_test.shape, y_test.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train and y_train : ((7352, 561),(7352,))\n",
            "X_test  and y_test  : ((2947, 561),(2947,))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pjUApGDdwWvW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Let's model with our data"
      ]
    },
    {
      "metadata": {
        "id": "aNOr1a7X5Oda",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Activities are the class labels\n",
        "# It is a 6 class classification\n",
        "ACTIVITIES = {\n",
        "    0: 'WALKING',\n",
        "    1: 'WALKING_UPSTAIRS',\n",
        "    2: 'WALKING_DOWNSTAIRS',\n",
        "    3: 'SITTING',\n",
        "    4: 'STANDING',\n",
        "    5: 'LAYING',\n",
        "}\n",
        "\n",
        "# Utility function to print the confusion matrix\n",
        "def confusion_matrix(Y_true, Y_pred):\n",
        "    Y_true = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_true, axis=1)])\n",
        "    Y_pred = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_pred, axis=1)])\n",
        "\n",
        "    return pd.crosstab(Y_true, Y_pred, rownames=['True'], colnames=['Pred'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RwhoGH_45QfA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Raw data signals\n",
        "# Signals are from Accelerometer and Gyroscope\n",
        "# The signals are in x,y,z directions\n",
        "# Sensor signals are filtered to have only body acceleration\n",
        "# excluding the acceleration due to gravity\n",
        "# Triaxial acceleration from the accelerometer is total acceleration\n",
        "SIGNALS = [\n",
        "    \"body_acc_x\",\n",
        "    \"body_acc_y\",\n",
        "    \"body_acc_z\",\n",
        "    \"body_gyro_x\",\n",
        "    \"body_gyro_y\",\n",
        "    \"body_gyro_z\",\n",
        "    \"total_acc_x\",\n",
        "    \"total_acc_y\",\n",
        "    \"total_acc_z\"\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x2bQHm0Z5T6w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Data directory\n",
        "DATADIR = 'UCI_HAR_Dataset'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T229tLHmw0gm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### functions for reading a file"
      ]
    },
    {
      "metadata": {
        "id": "8QI91MTe5Vv3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Utility function to read the data from csv file\n",
        "def _read_csv(filename):\n",
        "    return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
        "\n",
        "# Utility function to load the load\n",
        "def load_signals(subset):\n",
        "    signals_data = []\n",
        "\n",
        "    for signal in SIGNALS:\n",
        "        filename = f'{signal}_{subset}.txt'\n",
        "        signals_data.append(\n",
        "            _read_csv(filename).as_matrix()\n",
        "        ) \n",
        "\n",
        "    # Transpose is used to change the dimensionality of the output,\n",
        "    # aggregating the signals by combination of sample/timestep.\n",
        "    # Resultant shape is (7352 train/2947 test samples, 128 timesteps, 9 signals)\n",
        "    return np.transpose(signals_data, (1, 2, 0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RnMNGp3U5XY0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def load_y(subset):\n",
        "    \"\"\"\n",
        "    The objective that we are trying to predict is a integer, from 1 to 6,\n",
        "    that represents a human activity. We return a binary representation of \n",
        "    every sample objective as a 6 bits vector using One Hot Encoding\n",
        "    (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
        "    \"\"\"\n",
        "    filename = f'y_{subset}.txt'\n",
        "    y = _read_csv(filename)[0]\n",
        "\n",
        "    return pd.get_dummies(y).as_matrix()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D2T834i35uFP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp  -r \"/content/drive/My Drive/UCI_HAR_Dataset/test/Inertial Signals/body_acc_x_test.txt\" \"body_acc_x_test.txt\"\n",
        "!cp  -r \"/content/drive/My Drive/UCI_HAR_Dataset/test/Inertial Signals/body_acc_y_test.txt\" \"body_acc_y_test.txt\"\n",
        "!cp  -r \"/content/drive/My Drive/UCI_HAR_Dataset/test/Inertial Signals/body_acc_z_test.txt\" \"body_acc_z_test.txt\"\n",
        "!cp  -r \"/content/drive/My Drive/UCI_HAR_Dataset/test/Inertial Signals/body_gyro_x_test.txt\" \"body_gyro_x_test.txt\"\n",
        "!cp  -r \"/content/drive/My Drive/UCI_HAR_Dataset/test/Inertial Signals/body_gyro_y_test.txt\" \"body_gyro_y_test.txt\"\n",
        "!cp  -r \"/content/drive/My Drive/UCI_HAR_Dataset/test/Inertial Signals/body_gyro_z_test.txt\" \"body_gyro_z_test.txt\"\n",
        "!cp  -r \"/content/drive/My Drive/UCI_HAR_Dataset/test/Inertial Signals/total_acc_x_test.txt\" \"total_acc_x_test.txt\"\n",
        "!cp  -r \"/content/drive/My Drive/UCI_HAR_Dataset/test/Inertial Signals/total_acc_y_test.txt\" \"total_acc_y_test.txt\"\n",
        "!cp  -r \"/content/drive/My Drive/UCI_HAR_Dataset/test/Inertial Signals/total_acc_z_test.txt\" \"total_acc_z_test.txt\"\n",
        "\n",
        "\n",
        "!cp  -r \"/content/drive/My Drive/UCI_HAR_Dataset/train/Inertial Signals/body_acc_x_train.txt\" \"body_acc_x_train.txt\"\n",
        "!cp  -r \"/content/drive/My Drive/UCI_HAR_Dataset/train/Inertial Signals/body_acc_y_train.txt\" \"body_acc_y_train.txt\"\n",
        "!cp  -r \"/content/drive/My Drive/UCI_HAR_Dataset/train/Inertial Signals/body_acc_z_train.txt\" \"body_acc_z_train.txt\"\n",
        "!cp  -r \"/content/drive/My Drive/UCI_HAR_Dataset/train/Inertial Signals/body_gyro_x_train.txt\" \"body_gyro_x_train.txt\"\n",
        "!cp  -r \"/content/drive/My Drive/UCI_HAR_Dataset/train/Inertial Signals/body_gyro_y_train.txt\" \"body_gyro_y_train.txt\"\n",
        "!cp  -r \"/content/drive/My Drive/UCI_HAR_Dataset/train/Inertial Signals/body_gyro_z_train.txt\" \"body_gyro_z_train.txt\"\n",
        "!cp  -r \"/content/drive/My Drive/UCI_HAR_Dataset/train/Inertial Signals/total_acc_x_train.txt\" \"total_acc_x_train.txt\"\n",
        "!cp  -r \"/content/drive/My Drive/UCI_HAR_Dataset/train/Inertial Signals/total_acc_y_train.txt\" \"total_acc_y_train.txt\"\n",
        "!cp  -r \"/content/drive/My Drive/UCI_HAR_Dataset/train/Inertial Signals/total_acc_z_train.txt\" \"total_acc_z_train.txt\"\n",
        "\n",
        "\n",
        "!cp  -r \"/content/drive/My Drive/UCI_HAR_Dataset/train/X_train.txt\" \"X_train.txt\"\n",
        "!cp  -r \"/content/drive/My Drive/UCI_HAR_Dataset/test/X_test.txt\" \"X_test.txt\"\n",
        "!cp  -r \"/content/drive/My Drive/UCI_HAR_Dataset/test/y_test.txt\" \"y_test.txt\"\n",
        "!cp  -r \"/content/drive/My Drive/UCI_HAR_Dataset/train/y_train.txt\" \"y_train.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nK_9eJ3ryMUV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Utility function to count the number of classes\n",
        "def _count_classes(y):\n",
        "    return len(set([tuple(category) for category in y]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fH-sqC_Z5aKz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "  \n",
        "    SIGNALS = [\n",
        "    \"body_acc_x\",\n",
        "    \"body_acc_y\",\n",
        "    \"body_acc_z\",\n",
        "    \"body_gyro_x\",\n",
        "    \"body_gyro_y\",\n",
        "    \"body_gyro_z\",\n",
        "    \"total_acc_x\",\n",
        "    \"total_acc_y\",\n",
        "    \"total_acc_z\"\n",
        "    ]\n",
        "  \n",
        "    \n",
        "    def _read_csv(filename):\n",
        "        return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
        "\n",
        "    \n",
        "    def load_signals(subset):\n",
        "        signals_data = []\n",
        "        for signal in SIGNALS:\n",
        "            filename = f'{signal}_{subset}.txt'\n",
        "            signals_data.append(\n",
        "                _read_csv(filename).as_matrix()\n",
        "            ) \n",
        "        return np.transpose(signals_data, (1, 2, 0))\n",
        "\n",
        "    def load_y(subset):\n",
        "        filename = f'y_{subset}.txt'\n",
        "        y = _read_csv(filename)[0]\n",
        "\n",
        "        return pd.get_dummies(y).as_matrix()\n",
        "  \n",
        "  \n",
        "    X_train, X_test = load_signals('train'), load_signals('test')\n",
        "    y_train, y_test = load_y('train'), load_y('test')\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SN_ZW3ns5cq_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " x_train, x_test, y_train, y_test= load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X6RGG1mz5hWa",
        "colab_type": "code",
        "outputId": "8d5d6daf-4917-4dca-e6e4-b3294bdea6c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Importing tensorflow\n",
        "np.random.seed(42)\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# Configuring a session\n",
        "session_conf = tf.ConfigProto(\n",
        "    intra_op_parallelism_threads=1,\n",
        "    inter_op_parallelism_threads=1\n",
        ")\n",
        "\n",
        "\n",
        "# Import Keras\n",
        "from keras import backend as K\n",
        "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)\n",
        "\n",
        "\n",
        "# Importing libraries\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.core import Dense, Dropout\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from keras.layers import LSTM\n",
        "from hyperopt import Trials, STATUS_OK, tpe\n",
        "from keras.datasets import mnist\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.models import Sequential\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "d67jjI50xZLQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## LSTM MOdels"
      ]
    },
    {
      "metadata": {
        "id": "FJ3bmvQex7nT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Initializing parameters\n",
        "epochs = 30\n",
        "batch_size = 16\n",
        "n_hidden = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-7ga7CiAyAV9",
        "colab_type": "code",
        "outputId": "bedc6f3d-ccf0-4a05-c2bf-87cf7d32c7ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "timesteps = len(x_train[0])\n",
        "input_dim = len(x_train[0][0])\n",
        "n_classes = _count_classes(y_train)\n",
        "\n",
        "print(timesteps)\n",
        "print(input_dim)\n",
        "print(len(X_train))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "128\n",
            "9\n",
            "7352\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ujrTKFHJyhiH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Model 1 :: Defining the Architecture of LSTM\n",
        "#### LSTM ==>  Batchsize(16) - n_hidden(32) - epochs(30) - optimizer('rmsprop')"
      ]
    },
    {
      "metadata": {
        "id": "sF-F8qDMxo8w",
        "colab_type": "code",
        "outputId": "5964d1c9-70fb-44d8-f0f5-9ae9c04ebeee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "cell_type": "code",
      "source": [
        "# Initiliazing the sequential model\n",
        "model = Sequential()\n",
        "# Configuring the parameters\n",
        "model.add(LSTM(n_hidden, input_shape=(timesteps, input_dim)))\n",
        "# Adding a dropout layer\n",
        "model.add(Dropout(0.5))\n",
        "# Adding a dense output layer with sigmoid activation\n",
        "model.add(Dense(n_classes, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 32)                5376      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 6)                 198       \n",
            "=================================================================\n",
            "Total params: 5,574\n",
            "Trainable params: 5,574\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TKGC1Bllymco",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Compiling Model"
      ]
    },
    {
      "metadata": {
        "id": "EyxG8Hm-xtuO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Compiling the model\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LvjJVaIJytIe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Fit the model with data"
      ]
    },
    {
      "metadata": {
        "id": "8wNQyQGVx0Tm",
        "colab_type": "code",
        "outputId": "ee941fe3-fe10-4e1e-e18b-2fb44923b4c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1142
        }
      },
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          batch_size=batch_size,\n",
        "          validation_data=(x_test, y_test),\n",
        "          epochs=epochs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 7352 samples, validate on 2947 samples\n",
            "Epoch 1/30\n",
            "7352/7352 [==============================] - 32s 4ms/step - loss: 1.3407 - acc: 0.4446 - val_loss: 1.2371 - val_acc: 0.4499\n",
            "Epoch 2/30\n",
            "7352/7352 [==============================] - 30s 4ms/step - loss: 1.0163 - acc: 0.5499 - val_loss: 0.9499 - val_acc: 0.5786\n",
            "Epoch 3/30\n",
            "7352/7352 [==============================] - 30s 4ms/step - loss: 0.8186 - acc: 0.6439 - val_loss: 0.7760 - val_acc: 0.6159\n",
            "Epoch 4/30\n",
            "7352/7352 [==============================] - 30s 4ms/step - loss: 0.6821 - acc: 0.6877 - val_loss: 0.7513 - val_acc: 0.6922\n",
            "Epoch 5/30\n",
            "7352/7352 [==============================] - 31s 4ms/step - loss: 0.6108 - acc: 0.7323 - val_loss: 0.7069 - val_acc: 0.7190\n",
            "Epoch 6/30\n",
            "7352/7352 [==============================] - 31s 4ms/step - loss: 0.5607 - acc: 0.7678 - val_loss: 0.7870 - val_acc: 0.6675\n",
            "Epoch 7/30\n",
            "7352/7352 [==============================] - 32s 4ms/step - loss: 0.5035 - acc: 0.7930 - val_loss: 0.7386 - val_acc: 0.7122\n",
            "Epoch 8/30\n",
            "7352/7352 [==============================] - 31s 4ms/step - loss: 0.4477 - acc: 0.8232 - val_loss: 0.5959 - val_acc: 0.7852\n",
            "Epoch 9/30\n",
            "7352/7352 [==============================] - 30s 4ms/step - loss: 0.4027 - acc: 0.8589 - val_loss: 0.5584 - val_acc: 0.8514\n",
            "Epoch 10/30\n",
            "7352/7352 [==============================] - 31s 4ms/step - loss: 0.3463 - acc: 0.8931 - val_loss: 0.5884 - val_acc: 0.8378\n",
            "Epoch 11/30\n",
            "7352/7352 [==============================] - 30s 4ms/step - loss: 0.2659 - acc: 0.9183 - val_loss: 0.5026 - val_acc: 0.8741\n",
            "Epoch 12/30\n",
            "7352/7352 [==============================] - 30s 4ms/step - loss: 0.2437 - acc: 0.9259 - val_loss: 0.4746 - val_acc: 0.8782\n",
            "Epoch 13/30\n",
            "7352/7352 [==============================] - 31s 4ms/step - loss: 0.2204 - acc: 0.9291 - val_loss: 0.5735 - val_acc: 0.8741\n",
            "Epoch 14/30\n",
            "7352/7352 [==============================] - 31s 4ms/step - loss: 0.2325 - acc: 0.9308 - val_loss: 0.5100 - val_acc: 0.8870\n",
            "Epoch 15/30\n",
            "7352/7352 [==============================] - 31s 4ms/step - loss: 0.1982 - acc: 0.9340 - val_loss: 0.4421 - val_acc: 0.9023\n",
            "Epoch 16/30\n",
            "7352/7352 [==============================] - 31s 4ms/step - loss: 0.1995 - acc: 0.9361 - val_loss: 0.4106 - val_acc: 0.8951\n",
            "Epoch 17/30\n",
            "7352/7352 [==============================] - 31s 4ms/step - loss: 0.1891 - acc: 0.9397 - val_loss: 0.4351 - val_acc: 0.8911\n",
            "Epoch 18/30\n",
            "7352/7352 [==============================] - 31s 4ms/step - loss: 0.1793 - acc: 0.9442 - val_loss: 0.5344 - val_acc: 0.8741\n",
            "Epoch 19/30\n",
            "7352/7352 [==============================] - 31s 4ms/step - loss: 0.1826 - acc: 0.9384 - val_loss: 0.3620 - val_acc: 0.9002\n",
            "Epoch 20/30\n",
            "7352/7352 [==============================] - 30s 4ms/step - loss: 0.1699 - acc: 0.9430 - val_loss: 0.5131 - val_acc: 0.8643\n",
            "Epoch 21/30\n",
            "7352/7352 [==============================] - 31s 4ms/step - loss: 0.1635 - acc: 0.9411 - val_loss: 0.3672 - val_acc: 0.8951\n",
            "Epoch 22/30\n",
            "7352/7352 [==============================] - 30s 4ms/step - loss: 0.1696 - acc: 0.9421 - val_loss: 0.3934 - val_acc: 0.9043\n",
            "Epoch 23/30\n",
            "7352/7352 [==============================] - 30s 4ms/step - loss: 0.1689 - acc: 0.9453 - val_loss: 0.4160 - val_acc: 0.8921\n",
            "Epoch 24/30\n",
            "7352/7352 [==============================] - 30s 4ms/step - loss: 0.1520 - acc: 0.9482 - val_loss: 0.3932 - val_acc: 0.8972\n",
            "Epoch 25/30\n",
            "7352/7352 [==============================] - 30s 4ms/step - loss: 0.1543 - acc: 0.9486 - val_loss: 0.4313 - val_acc: 0.8938\n",
            "Epoch 26/30\n",
            "7352/7352 [==============================] - 31s 4ms/step - loss: 0.1718 - acc: 0.9444 - val_loss: 0.4254 - val_acc: 0.8928\n",
            "Epoch 27/30\n",
            "7352/7352 [==============================] - 31s 4ms/step - loss: 0.1620 - acc: 0.9484 - val_loss: 0.4403 - val_acc: 0.8894\n",
            "Epoch 28/30\n",
            "7352/7352 [==============================] - 32s 4ms/step - loss: 0.1551 - acc: 0.9476 - val_loss: 0.4360 - val_acc: 0.8962\n",
            "Epoch 29/30\n",
            "7352/7352 [==============================] - 31s 4ms/step - loss: 0.1505 - acc: 0.9467 - val_loss: 0.4254 - val_acc: 0.9046\n",
            "Epoch 30/30\n",
            "7352/7352 [==============================] - 31s 4ms/step - loss: 0.1685 - acc: 0.9483 - val_loss: 0.5401 - val_acc: 0.8785\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7b40cb5cf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "cIiodYMNxsde",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### MOdel2 ::  Defining the Architecture of LSTM\n",
        "#### LSTM ==>  Batchsize(128) - n_hidden(32) - epochs(30) - Dropout(0.7) -  optimizer(Adam(lr=1e-3))"
      ]
    },
    {
      "metadata": {
        "id": "nugxXZym6Xc7",
        "colab_type": "code",
        "outputId": "153284a7-d6ec-449a-f8a6-be4e1e7c222f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1244
        }
      },
      "cell_type": "code",
      "source": [
        "#'categorical_crossentropy'\n",
        "model = Sequential()\n",
        "model.add(LSTM(32, input_shape=(128,9)))\n",
        "model.add(Dropout(0.7))\n",
        "#model.add(LSTM(32))\n",
        "model.add(Dense(6, activation='sigmoid'))\n",
        "        \n",
        "adam=Adam(lr=1e-3)\n",
        "model.compile(optimizer=adam , loss='categorical_crossentropy' ,metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,batch_size=128,epochs=30,validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 5881 samples, validate on 1471 samples\n",
            "Epoch 1/30\n",
            "5881/5881 [==============================] - 7s 1ms/step - loss: 1.7082 - acc: 0.3253 - val_loss: 1.5886 - val_acc: 0.4058\n",
            "Epoch 2/30\n",
            "5881/5881 [==============================] - 6s 956us/step - loss: 1.4924 - acc: 0.3969 - val_loss: 1.3854 - val_acc: 0.3929\n",
            "Epoch 3/30\n",
            "5881/5881 [==============================] - 6s 955us/step - loss: 1.3622 - acc: 0.4435 - val_loss: 1.2874 - val_acc: 0.4208\n",
            "Epoch 4/30\n",
            "5881/5881 [==============================] - 6s 949us/step - loss: 1.3107 - acc: 0.4557 - val_loss: 1.2388 - val_acc: 0.4765\n",
            "Epoch 5/30\n",
            "5881/5881 [==============================] - 6s 940us/step - loss: 1.2515 - acc: 0.4827 - val_loss: 1.2165 - val_acc: 0.4562\n",
            "Epoch 6/30\n",
            "5881/5881 [==============================] - 6s 950us/step - loss: 1.1988 - acc: 0.4766 - val_loss: 1.1709 - val_acc: 0.4697\n",
            "Epoch 7/30\n",
            "5881/5881 [==============================] - 6s 955us/step - loss: 1.2546 - acc: 0.4363 - val_loss: 1.3283 - val_acc: 0.3481\n",
            "Epoch 8/30\n",
            "5881/5881 [==============================] - 6s 972us/step - loss: 1.2355 - acc: 0.4411 - val_loss: 1.1866 - val_acc: 0.3977\n",
            "Epoch 9/30\n",
            "5881/5881 [==============================] - 5s 935us/step - loss: 1.1492 - acc: 0.5132 - val_loss: 1.1264 - val_acc: 0.5235\n",
            "Epoch 10/30\n",
            "5881/5881 [==============================] - 6s 964us/step - loss: 1.0861 - acc: 0.5576 - val_loss: 1.0474 - val_acc: 0.5758\n",
            "Epoch 11/30\n",
            "5881/5881 [==============================] - 6s 1ms/step - loss: 1.0471 - acc: 0.5615 - val_loss: 1.0352 - val_acc: 0.5547\n",
            "Epoch 12/30\n",
            "5881/5881 [==============================] - 6s 953us/step - loss: 1.0442 - acc: 0.5412 - val_loss: 1.0913 - val_acc: 0.5384\n",
            "Epoch 13/30\n",
            "5881/5881 [==============================] - 6s 951us/step - loss: 1.0465 - acc: 0.5428 - val_loss: 1.0911 - val_acc: 0.5574\n",
            "Epoch 14/30\n",
            "5881/5881 [==============================] - 6s 947us/step - loss: 1.0228 - acc: 0.5355 - val_loss: 1.0532 - val_acc: 0.5282\n",
            "Epoch 15/30\n",
            "5881/5881 [==============================] - 6s 944us/step - loss: 1.0278 - acc: 0.5304 - val_loss: 1.0790 - val_acc: 0.5139\n",
            "Epoch 16/30\n",
            "5881/5881 [==============================] - 6s 936us/step - loss: 1.0180 - acc: 0.5288 - val_loss: 1.0557 - val_acc: 0.5404\n",
            "Epoch 17/30\n",
            "5881/5881 [==============================] - 6s 953us/step - loss: 1.0629 - acc: 0.5271 - val_loss: 1.2405 - val_acc: 0.4854\n",
            "Epoch 18/30\n",
            "5881/5881 [==============================] - 6s 940us/step - loss: 1.1521 - acc: 0.4982 - val_loss: 1.2217 - val_acc: 0.4779\n",
            "Epoch 19/30\n",
            "5881/5881 [==============================] - 6s 945us/step - loss: 1.0763 - acc: 0.5088 - val_loss: 1.0951 - val_acc: 0.5194\n",
            "Epoch 20/30\n",
            "5881/5881 [==============================] - 6s 951us/step - loss: 1.0137 - acc: 0.5338 - val_loss: 1.0105 - val_acc: 0.5840\n",
            "Epoch 21/30\n",
            "5881/5881 [==============================] - 6s 974us/step - loss: 0.9950 - acc: 0.5463 - val_loss: 1.0115 - val_acc: 0.6111\n",
            "Epoch 22/30\n",
            "5881/5881 [==============================] - 6s 960us/step - loss: 1.0801 - acc: 0.5099 - val_loss: 1.2148 - val_acc: 0.4616\n",
            "Epoch 23/30\n",
            "5881/5881 [==============================] - 6s 966us/step - loss: 1.0686 - acc: 0.5113 - val_loss: 1.0680 - val_acc: 0.5085\n",
            "Epoch 24/30\n",
            "5881/5881 [==============================] - 6s 953us/step - loss: 1.0231 - acc: 0.5343 - val_loss: 1.0435 - val_acc: 0.5459\n",
            "Epoch 25/30\n",
            "5881/5881 [==============================] - 6s 956us/step - loss: 1.0094 - acc: 0.5361 - val_loss: 1.0361 - val_acc: 0.5506\n",
            "Epoch 26/30\n",
            "5881/5881 [==============================] - 6s 979us/step - loss: 1.0008 - acc: 0.5331 - val_loss: 1.0292 - val_acc: 0.5595\n",
            "Epoch 27/30\n",
            "5881/5881 [==============================] - 6s 980us/step - loss: 0.9902 - acc: 0.5443 - val_loss: 1.0226 - val_acc: 0.5690\n",
            "Epoch 28/30\n",
            "5881/5881 [==============================] - 6s 981us/step - loss: 0.9829 - acc: 0.5511 - val_loss: 1.0275 - val_acc: 0.5731\n",
            "Epoch 29/30\n",
            "5881/5881 [==============================] - 6s 967us/step - loss: 0.9918 - acc: 0.5491 - val_loss: 1.0382 - val_acc: 0.5744\n",
            "Epoch 30/30\n",
            "5881/5881 [==============================] - 6s 966us/step - loss: 0.9776 - acc: 0.5531 - val_loss: 1.0182 - val_acc: 0.5744\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcb6598ee10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "jpx9PoPKz1gR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Model 3 :\n",
        "### Defining the Architecture of LSTM\n",
        "#### LSTM ==> Batchsize(8) - n_hidden(32) - epochs(30) - Dropout(0.7) - optimizer(Adam(lr=1e-3))"
      ]
    },
    {
      "metadata": {
        "id": "7ck9zhcl7B3o",
        "colab_type": "code",
        "outputId": "a3f3d7f8-709a-43fc-8233-1ae81d669fb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1071
        }
      },
      "cell_type": "code",
      "source": [
        "#'categorical_crossentropy'\n",
        "model = Sequential()\n",
        "model.add(LSTM(32, input_shape=(128,9)))\n",
        "model.add(Dropout(0.7))\n",
        "#model.add(LSTM(32))\n",
        "model.add(Dense(6, activation='sigmoid'))\n",
        "        \n",
        "adam=Adam(lr=1e-3)\n",
        "model.compile(optimizer=adam , loss='categorical_crossentropy' ,metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,batch_size=8,epochs=30,validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 5881 samples, validate on 1471 samples\n",
            "Epoch 1/30\n",
            "5881/5881 [==============================] - 45s 8ms/step - loss: 1.3928 - acc: 0.4027 - val_loss: 1.1749 - val_acc: 0.4908\n",
            "Epoch 2/30\n",
            "5881/5881 [==============================] - 45s 8ms/step - loss: 1.2162 - acc: 0.4669 - val_loss: 1.2209 - val_acc: 0.4398\n",
            "Epoch 3/30\n",
            "5881/5881 [==============================] - 44s 8ms/step - loss: 1.2488 - acc: 0.4520 - val_loss: 1.1821 - val_acc: 0.5105\n",
            "Epoch 4/30\n",
            "5881/5881 [==============================] - 44s 7ms/step - loss: 1.0865 - acc: 0.5037 - val_loss: 1.0114 - val_acc: 0.5744\n",
            "Epoch 5/30\n",
            "5881/5881 [==============================] - 44s 8ms/step - loss: 1.0081 - acc: 0.5453 - val_loss: 0.9720 - val_acc: 0.5772\n",
            "Epoch 6/30\n",
            "5881/5881 [==============================] - 43s 7ms/step - loss: 0.8947 - acc: 0.5984 - val_loss: 0.9200 - val_acc: 0.5472\n",
            "Epoch 7/30\n",
            "5881/5881 [==============================] - 44s 7ms/step - loss: 0.8068 - acc: 0.6446 - val_loss: 0.8239 - val_acc: 0.6349\n",
            "Epoch 8/30\n",
            "5881/5881 [==============================] - 44s 7ms/step - loss: 0.7787 - acc: 0.6564 - val_loss: 0.8456 - val_acc: 0.6268\n",
            "Epoch 9/30\n",
            "5881/5881 [==============================] - 44s 7ms/step - loss: 0.8227 - acc: 0.6348 - val_loss: 0.9181 - val_acc: 0.6084\n",
            "Epoch 10/30\n",
            "5881/5881 [==============================] - 44s 7ms/step - loss: 0.8778 - acc: 0.6079 - val_loss: 0.9943 - val_acc: 0.5955\n",
            "Epoch 11/30\n",
            "5881/5881 [==============================] - 43s 7ms/step - loss: 0.8129 - acc: 0.6196 - val_loss: 0.9104 - val_acc: 0.6139\n",
            "Epoch 12/30\n",
            "5881/5881 [==============================] - 43s 7ms/step - loss: 0.7272 - acc: 0.6545 - val_loss: 0.8843 - val_acc: 0.6132\n",
            "Epoch 13/30\n",
            "5881/5881 [==============================] - 45s 8ms/step - loss: 0.9308 - acc: 0.5888 - val_loss: 0.9767 - val_acc: 0.6016\n",
            "Epoch 14/30\n",
            "5881/5881 [==============================] - 44s 7ms/step - loss: 0.7175 - acc: 0.6586 - val_loss: 1.0015 - val_acc: 0.5996\n",
            "Epoch 15/30\n",
            "5881/5881 [==============================] - 43s 7ms/step - loss: 0.6830 - acc: 0.6652 - val_loss: 0.9470 - val_acc: 0.6098\n",
            "Epoch 16/30\n",
            "5881/5881 [==============================] - 43s 7ms/step - loss: 0.6851 - acc: 0.6620 - val_loss: 1.1741 - val_acc: 0.5778\n",
            "Epoch 17/30\n",
            "5881/5881 [==============================] - 44s 8ms/step - loss: 0.6497 - acc: 0.6946 - val_loss: 0.8996 - val_acc: 0.6111\n",
            "Epoch 18/30\n",
            "5881/5881 [==============================] - 44s 7ms/step - loss: 0.6564 - acc: 0.6822 - val_loss: 0.9150 - val_acc: 0.5935\n",
            "Epoch 19/30\n",
            "5881/5881 [==============================] - 44s 8ms/step - loss: 0.6327 - acc: 0.6898 - val_loss: 0.8584 - val_acc: 0.6363\n",
            "Epoch 20/30\n",
            "5881/5881 [==============================] - 44s 7ms/step - loss: 0.6201 - acc: 0.7114 - val_loss: 0.8677 - val_acc: 0.6295\n",
            "Epoch 21/30\n",
            "5881/5881 [==============================] - 43s 7ms/step - loss: 0.7752 - acc: 0.6803 - val_loss: 0.8598 - val_acc: 0.6499\n",
            "Epoch 22/30\n",
            "5881/5881 [==============================] - 43s 7ms/step - loss: 0.6201 - acc: 0.7239 - val_loss: 0.8530 - val_acc: 0.7104\n",
            "Epoch 23/30\n",
            "5881/5881 [==============================] - 43s 7ms/step - loss: 0.5648 - acc: 0.7601 - val_loss: 0.8330 - val_acc: 0.7260\n",
            "Epoch 24/30\n",
            "5881/5881 [==============================] - 42s 7ms/step - loss: 0.5693 - acc: 0.7551 - val_loss: 0.8711 - val_acc: 0.7818\n",
            "Epoch 25/30\n",
            "5881/5881 [==============================] - 43s 7ms/step - loss: 0.5659 - acc: 0.7847 - val_loss: 0.8535 - val_acc: 0.7811\n",
            "Epoch 26/30\n",
            "5881/5881 [==============================] - 43s 7ms/step - loss: 0.5545 - acc: 0.8063 - val_loss: 0.8992 - val_acc: 0.8008\n",
            "Epoch 27/30\n",
            "5881/5881 [==============================] - 43s 7ms/step - loss: 0.5395 - acc: 0.8332 - val_loss: 0.6885 - val_acc: 0.8579\n",
            "Epoch 28/30\n",
            "5881/5881 [==============================] - 42s 7ms/step - loss: 0.4328 - acc: 0.8592 - val_loss: 0.6666 - val_acc: 0.8572\n",
            "Epoch 29/30\n",
            "5881/5881 [==============================] - 43s 7ms/step - loss: 0.3925 - acc: 0.8828 - val_loss: 0.5740 - val_acc: 0.8756\n",
            "Epoch 30/30\n",
            "5881/5881 [==============================] - 43s 7ms/step - loss: 0.3341 - acc: 0.9022 - val_loss: 0.5777 - val_acc: 0.8708\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcb64544a58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "eAf63F6k0K8T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Tuning on LSTM Model using  TALOS Library"
      ]
    },
    {
      "metadata": {
        "id": "MwRF7UCOBf7g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install talos\n",
        "import talos as ta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "THv8xGu08WJQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_model(x_train, y_train, x_test, y_test, params):\n",
        "        \n",
        "        model = Sequential()\n",
        "        model.add(LSTM(params['units'],return_sequences=True, input_shape=(128,9)))\n",
        "        model.add(Dropout(params['Dropout']))\n",
        "        model.add(LSTM(32))\n",
        "        model.add(Dense(6, activation='sigmoid'))\n",
        "        \n",
        "        adam=Adam(lr=1e-3)\n",
        "        model.compile(optimizer=adam , loss='categorical_crossentropy' ,metrics=['accuracy'])\n",
        "\n",
        "        out = model.fit(x_train, y_train,batch_size=16,epochs=30,validation_data=(x_test, y_test))\n",
        "\n",
        "        return out, model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "511eFwsJAM00",
        "colab_type": "code",
        "outputId": "07f345f6-3904-4e06-ef80-10fcf9cfffb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9537
        }
      },
      "cell_type": "code",
      "source": [
        "p = {'units' : [32,64,128],\n",
        "         'Dropout' : [0.5,0.7,0.8] \n",
        "     } \n",
        "\n",
        "scan_object = ta.Scan(x_train, y_train, params=p, model=create_model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/9 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 5146 samples, validate on 2206 samples\n",
            "Epoch 1/30\n",
            "5146/5146 [==============================] - 49s 10ms/step - loss: 1.3987 - acc: 0.3747 - val_loss: 1.3188 - val_acc: 0.4524\n",
            "Epoch 2/30\n",
            "5146/5146 [==============================] - 47s 9ms/step - loss: 1.2715 - acc: 0.4279 - val_loss: 1.3722 - val_acc: 0.3259\n",
            "Epoch 3/30\n",
            "5146/5146 [==============================] - 47s 9ms/step - loss: 1.3612 - acc: 0.3671 - val_loss: 1.3402 - val_acc: 0.4937\n",
            "Epoch 4/30\n",
            "5146/5146 [==============================] - 47s 9ms/step - loss: 1.3367 - acc: 0.3760 - val_loss: 1.3218 - val_acc: 0.3604\n",
            "Epoch 5/30\n",
            "5146/5146 [==============================] - 47s 9ms/step - loss: 1.3010 - acc: 0.4155 - val_loss: 1.2233 - val_acc: 0.4651\n",
            "Epoch 6/30\n",
            "5146/5146 [==============================] - 48s 9ms/step - loss: 1.0864 - acc: 0.5346 - val_loss: 0.8135 - val_acc: 0.6704\n",
            "Epoch 7/30\n",
            "5146/5146 [==============================] - 47s 9ms/step - loss: 0.7582 - acc: 0.6504 - val_loss: 0.6672 - val_acc: 0.6677\n",
            "Epoch 8/30\n",
            "5146/5146 [==============================] - 47s 9ms/step - loss: 0.6463 - acc: 0.7087 - val_loss: 0.6427 - val_acc: 0.7108\n",
            "Epoch 9/30\n",
            "5146/5146 [==============================] - 47s 9ms/step - loss: 0.6214 - acc: 0.7256 - val_loss: 0.6054 - val_acc: 0.7353\n",
            "Epoch 10/30\n",
            "5146/5146 [==============================] - 47s 9ms/step - loss: 0.5529 - acc: 0.7567 - val_loss: 0.5202 - val_acc: 0.7312\n",
            "Epoch 11/30\n",
            "5146/5146 [==============================] - 46s 9ms/step - loss: 0.4304 - acc: 0.8434 - val_loss: 0.3440 - val_acc: 0.8858\n",
            "Epoch 12/30\n",
            "5146/5146 [==============================] - 46s 9ms/step - loss: 0.2398 - acc: 0.9229 - val_loss: 0.2499 - val_acc: 0.9257\n",
            "Epoch 13/30\n",
            "5146/5146 [==============================] - 48s 9ms/step - loss: 0.2072 - acc: 0.9308 - val_loss: 0.2348 - val_acc: 0.8930\n",
            "Epoch 14/30\n",
            "5146/5146 [==============================] - 48s 9ms/step - loss: 0.1815 - acc: 0.9355 - val_loss: 0.1816 - val_acc: 0.9429\n",
            "Epoch 15/30\n",
            "5146/5146 [==============================] - 47s 9ms/step - loss: 0.1449 - acc: 0.9483 - val_loss: 0.1625 - val_acc: 0.9334\n",
            "Epoch 16/30\n",
            "5146/5146 [==============================] - 47s 9ms/step - loss: 0.1957 - acc: 0.9300 - val_loss: 0.1757 - val_acc: 0.9365\n",
            "Epoch 17/30\n",
            "5146/5146 [==============================] - 46s 9ms/step - loss: 0.1601 - acc: 0.9417 - val_loss: 0.2000 - val_acc: 0.9202\n",
            "Epoch 18/30\n",
            "5146/5146 [==============================] - 46s 9ms/step - loss: 0.1465 - acc: 0.9435 - val_loss: 0.1524 - val_acc: 0.9306\n",
            "Epoch 19/30\n",
            "5146/5146 [==============================] - 47s 9ms/step - loss: 0.1406 - acc: 0.9458 - val_loss: 0.1805 - val_acc: 0.9347\n",
            "Epoch 20/30\n",
            "5146/5146 [==============================] - 47s 9ms/step - loss: 0.1326 - acc: 0.9497 - val_loss: 0.1404 - val_acc: 0.9397\n",
            "Epoch 21/30\n",
            "5146/5146 [==============================] - 46s 9ms/step - loss: 0.1277 - acc: 0.9485 - val_loss: 0.1390 - val_acc: 0.9492\n",
            "Epoch 22/30\n",
            "5146/5146 [==============================] - 46s 9ms/step - loss: 0.1165 - acc: 0.9536 - val_loss: 0.1363 - val_acc: 0.9424\n",
            "Epoch 23/30\n",
            "5146/5146 [==============================] - 46s 9ms/step - loss: 0.1183 - acc: 0.9524 - val_loss: 0.1265 - val_acc: 0.9533\n",
            "Epoch 24/30\n",
            "5146/5146 [==============================] - 46s 9ms/step - loss: 0.1263 - acc: 0.9510 - val_loss: 0.5174 - val_acc: 0.8821\n",
            "Epoch 25/30\n",
            "5146/5146 [==============================] - 46s 9ms/step - loss: 0.1540 - acc: 0.9438 - val_loss: 0.1346 - val_acc: 0.9542\n",
            "Epoch 26/30\n",
            "5146/5146 [==============================] - 46s 9ms/step - loss: 0.1563 - acc: 0.9403 - val_loss: 0.1679 - val_acc: 0.9379\n",
            "Epoch 27/30\n",
            "5146/5146 [==============================] - 48s 9ms/step - loss: 0.1417 - acc: 0.9493 - val_loss: 0.1371 - val_acc: 0.9442\n",
            "Epoch 28/30\n",
            "5146/5146 [==============================] - 47s 9ms/step - loss: 0.1238 - acc: 0.9522 - val_loss: 0.1276 - val_acc: 0.9556\n",
            "Epoch 29/30\n",
            "5146/5146 [==============================] - 47s 9ms/step - loss: 0.1216 - acc: 0.9528 - val_loss: 0.1581 - val_acc: 0.9311\n",
            "Epoch 30/30\n",
            "5146/5146 [==============================] - 47s 9ms/step - loss: 0.1165 - acc: 0.9549 - val_loss: 0.1301 - val_acc: 0.9538\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 11%|█         | 1/9 [23:26<3:07:34, 1406.86s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 5146 samples, validate on 2206 samples\n",
            "Epoch 1/30\n",
            "5146/5146 [==============================] - 48s 9ms/step - loss: 1.3160 - acc: 0.4438 - val_loss: 1.1747 - val_acc: 0.4723\n",
            "Epoch 2/30\n",
            "5146/5146 [==============================] - 47s 9ms/step - loss: 1.0634 - acc: 0.5394 - val_loss: 1.0002 - val_acc: 0.5861\n",
            "Epoch 3/30\n",
            "5146/5146 [==============================] - 47s 9ms/step - loss: 0.9715 - acc: 0.5733 - val_loss: 0.8456 - val_acc: 0.5970\n",
            "Epoch 4/30\n",
            "5146/5146 [==============================] - 47s 9ms/step - loss: 0.6749 - acc: 0.7258 - val_loss: 0.5701 - val_acc: 0.7539\n",
            "Epoch 5/30\n",
            "5146/5146 [==============================] - 47s 9ms/step - loss: 0.4962 - acc: 0.7719 - val_loss: 0.7095 - val_acc: 0.6587\n",
            "Epoch 6/30\n",
            "5146/5146 [==============================] - 46s 9ms/step - loss: 0.4515 - acc: 0.8140 - val_loss: 0.3693 - val_acc: 0.9112\n",
            "Epoch 7/30\n",
            "5146/5146 [==============================] - 46s 9ms/step - loss: 0.2963 - acc: 0.9162 - val_loss: 0.2421 - val_acc: 0.9306\n",
            "Epoch 8/30\n",
            "5146/5146 [==============================] - 47s 9ms/step - loss: 0.3033 - acc: 0.8972 - val_loss: 0.2924 - val_acc: 0.9080\n",
            "Epoch 9/30\n",
            "5146/5146 [==============================] - 46s 9ms/step - loss: 0.2861 - acc: 0.9067 - val_loss: 0.2154 - val_acc: 0.9361\n",
            "Epoch 10/30\n",
            "5146/5146 [==============================] - 47s 9ms/step - loss: 0.2144 - acc: 0.9293 - val_loss: 0.2242 - val_acc: 0.9257\n",
            "Epoch 11/30\n",
            "5146/5146 [==============================] - 46s 9ms/step - loss: 0.2061 - acc: 0.9332 - val_loss: 0.1584 - val_acc: 0.9429\n",
            "Epoch 12/30\n",
            "5146/5146 [==============================] - 47s 9ms/step - loss: 0.1615 - acc: 0.9425 - val_loss: 0.1574 - val_acc: 0.9383\n",
            "Epoch 13/30\n",
            "5146/5146 [==============================] - 46s 9ms/step - loss: 0.1331 - acc: 0.9530 - val_loss: 0.1801 - val_acc: 0.9388\n",
            "Epoch 14/30\n",
            "5146/5146 [==============================] - 46s 9ms/step - loss: 0.1351 - acc: 0.9495 - val_loss: 0.1311 - val_acc: 0.9474\n",
            "Epoch 15/30\n",
            "5146/5146 [==============================] - 46s 9ms/step - loss: 0.1297 - acc: 0.9518 - val_loss: 0.1730 - val_acc: 0.9388\n",
            "Epoch 16/30\n",
            "5146/5146 [==============================] - 47s 9ms/step - loss: 0.1365 - acc: 0.9479 - val_loss: 0.2051 - val_acc: 0.9270\n",
            "Epoch 17/30\n",
            "5146/5146 [==============================] - 47s 9ms/step - loss: 0.1555 - acc: 0.9440 - val_loss: 0.1470 - val_acc: 0.9542\n",
            "Epoch 18/30\n",
            "5146/5146 [==============================] - 46s 9ms/step - loss: 0.1347 - acc: 0.9468 - val_loss: 0.2259 - val_acc: 0.9170\n",
            "Epoch 19/30\n",
            "5146/5146 [==============================] - 47s 9ms/step - loss: 0.1336 - acc: 0.9489 - val_loss: 0.1245 - val_acc: 0.9574\n",
            "Epoch 20/30\n",
            "5146/5146 [==============================] - 47s 9ms/step - loss: 0.1234 - acc: 0.9508 - val_loss: 0.1440 - val_acc: 0.9547\n",
            "Epoch 21/30\n",
            "5146/5146 [==============================] - 47s 9ms/step - loss: 0.1246 - acc: 0.9444 - val_loss: 0.1314 - val_acc: 0.9420\n",
            "Epoch 22/30\n",
            "5146/5146 [==============================] - 46s 9ms/step - loss: 0.1243 - acc: 0.9468 - val_loss: 0.1430 - val_acc: 0.9451\n",
            "Epoch 23/30\n",
            "5146/5146 [==============================] - 47s 9ms/step - loss: 0.1428 - acc: 0.9458 - val_loss: 0.1303 - val_acc: 0.9510\n",
            "Epoch 24/30\n",
            "5146/5146 [==============================] - 47s 9ms/step - loss: 0.1659 - acc: 0.9413 - val_loss: 0.1506 - val_acc: 0.9510\n",
            "Epoch 25/30\n",
            "5146/5146 [==============================] - 46s 9ms/step - loss: 0.1479 - acc: 0.9471 - val_loss: 0.1336 - val_acc: 0.9565\n",
            "Epoch 26/30\n",
            "5146/5146 [==============================] - 46s 9ms/step - loss: 0.1202 - acc: 0.9553 - val_loss: 0.1196 - val_acc: 0.9560\n",
            "Epoch 27/30\n",
            "5146/5146 [==============================] - 46s 9ms/step - loss: 0.1194 - acc: 0.9516 - val_loss: 0.1119 - val_acc: 0.9606\n",
            "Epoch 28/30\n",
            "5146/5146 [==============================] - 46s 9ms/step - loss: 0.1177 - acc: 0.9524 - val_loss: 0.1211 - val_acc: 0.9565\n",
            "Epoch 29/30\n",
            "5146/5146 [==============================] - 46s 9ms/step - loss: 0.1367 - acc: 0.9489 - val_loss: 0.1268 - val_acc: 0.9551\n",
            "Epoch 30/30\n",
            "5146/5146 [==============================] - 46s 9ms/step - loss: 0.1202 - acc: 0.9528 - val_loss: 0.1245 - val_acc: 0.9574\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 22%|██▏       | 2/9 [46:45<2:43:50, 1404.39s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 5146 samples, validate on 2206 samples\n",
            "Epoch 1/30\n",
            "5146/5146 [==============================] - 47s 9ms/step - loss: 1.2544 - acc: 0.4786 - val_loss: 0.9731 - val_acc: 0.5852\n",
            "Epoch 2/30\n",
            "5146/5146 [==============================] - 46s 9ms/step - loss: 0.8484 - acc: 0.6335 - val_loss: 0.7020 - val_acc: 0.6890\n",
            "Epoch 3/30\n",
            "5146/5146 [==============================] - 46s 9ms/step - loss: 0.7039 - acc: 0.6627 - val_loss: 0.7201 - val_acc: 0.6364\n",
            "Epoch 4/30\n",
            "5146/5146 [==============================] - 46s 9ms/step - loss: 0.6897 - acc: 0.6823 - val_loss: 0.6049 - val_acc: 0.7520\n",
            "Epoch 5/30\n",
            "5146/5146 [==============================] - 46s 9ms/step - loss: 0.5937 - acc: 0.7623 - val_loss: 0.4976 - val_acc: 0.8459\n",
            "Epoch 6/30\n",
            "5146/5146 [==============================] - 46s 9ms/step - loss: 0.5129 - acc: 0.7927 - val_loss: 0.4901 - val_acc: 0.7901\n",
            "Epoch 7/30\n",
            "5146/5146 [==============================] - 47s 9ms/step - loss: 0.3183 - acc: 0.8990 - val_loss: 0.2261 - val_acc: 0.9347\n",
            "Epoch 8/30\n",
            "5146/5146 [==============================] - 46s 9ms/step - loss: 0.2459 - acc: 0.9054 - val_loss: 0.1742 - val_acc: 0.9465\n",
            "Epoch 9/30\n",
            "5146/5146 [==============================] - 46s 9ms/step - loss: 0.2060 - acc: 0.9264 - val_loss: 0.1825 - val_acc: 0.9297\n",
            "Epoch 10/30\n",
            "5146/5146 [==============================] - 46s 9ms/step - loss: 0.1809 - acc: 0.9400 - val_loss: 0.1791 - val_acc: 0.9402\n",
            "Epoch 11/30\n",
            "5146/5146 [==============================] - 47s 9ms/step - loss: 0.1633 - acc: 0.9448 - val_loss: 0.1449 - val_acc: 0.9510\n",
            "Epoch 12/30\n",
            "5146/5146 [==============================] - 47s 9ms/step - loss: 0.1594 - acc: 0.9431 - val_loss: 0.1701 - val_acc: 0.9284\n",
            "Epoch 13/30\n",
            "5146/5146 [==============================] - 47s 9ms/step - loss: 0.1683 - acc: 0.9382 - val_loss: 0.1615 - val_acc: 0.9438\n",
            "Epoch 14/30\n",
            "5146/5146 [==============================] - 48s 9ms/step - loss: 0.1733 - acc: 0.9390 - val_loss: 0.1294 - val_acc: 0.9542\n",
            "Epoch 15/30\n",
            "5146/5146 [==============================] - 47s 9ms/step - loss: 0.1480 - acc: 0.9477 - val_loss: 0.1968 - val_acc: 0.9288\n",
            "Epoch 16/30\n",
            "5146/5146 [==============================] - 47s 9ms/step - loss: 0.1332 - acc: 0.9489 - val_loss: 0.1312 - val_acc: 0.9524\n",
            "Epoch 17/30\n",
            "5146/5146 [==============================] - 47s 9ms/step - loss: 0.1348 - acc: 0.9483 - val_loss: 0.1385 - val_acc: 0.9479\n",
            "Epoch 18/30\n",
            "5146/5146 [==============================] - 48s 9ms/step - loss: 0.1647 - acc: 0.9398 - val_loss: 0.1427 - val_acc: 0.9506\n",
            "Epoch 19/30\n",
            "5146/5146 [==============================] - 48s 9ms/step - loss: 0.1510 - acc: 0.9359 - val_loss: 0.2543 - val_acc: 0.8708\n",
            "Epoch 20/30\n",
            "5146/5146 [==============================] - 49s 9ms/step - loss: 0.1621 - acc: 0.9386 - val_loss: 0.1523 - val_acc: 0.9411\n",
            "Epoch 21/30\n",
            "5146/5146 [==============================] - 48s 9ms/step - loss: 0.1453 - acc: 0.9400 - val_loss: 0.1417 - val_acc: 0.9483\n",
            "Epoch 22/30\n",
            "5146/5146 [==============================] - 49s 9ms/step - loss: 0.1338 - acc: 0.9487 - val_loss: 0.1628 - val_acc: 0.9433\n",
            "Epoch 23/30\n",
            "5146/5146 [==============================] - 48s 9ms/step - loss: 0.1326 - acc: 0.9452 - val_loss: 0.1303 - val_acc: 0.9438\n",
            "Epoch 24/30\n",
            "5146/5146 [==============================] - 48s 9ms/step - loss: 0.1406 - acc: 0.9475 - val_loss: 0.1381 - val_acc: 0.9415\n",
            "Epoch 25/30\n",
            "5146/5146 [==============================] - 49s 9ms/step - loss: 0.1248 - acc: 0.9526 - val_loss: 0.1370 - val_acc: 0.9479\n",
            "Epoch 26/30\n",
            "5146/5146 [==============================] - 48s 9ms/step - loss: 0.1207 - acc: 0.9524 - val_loss: 0.1416 - val_acc: 0.9488\n",
            "Epoch 27/30\n",
            "5146/5146 [==============================] - 49s 9ms/step - loss: 0.1312 - acc: 0.9503 - val_loss: 0.1211 - val_acc: 0.9556\n",
            "Epoch 28/30\n",
            "5146/5146 [==============================] - 48s 9ms/step - loss: 0.1562 - acc: 0.9436 - val_loss: 0.1363 - val_acc: 0.9433\n",
            "Epoch 29/30\n",
            "5146/5146 [==============================] - 48s 9ms/step - loss: 0.1172 - acc: 0.9526 - val_loss: 0.1167 - val_acc: 0.9529\n",
            "Epoch 30/30\n",
            "5146/5146 [==============================] - 47s 9ms/step - loss: 0.1180 - acc: 0.9520 - val_loss: 0.1210 - val_acc: 0.9551\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 33%|███▎      | 3/9 [1:10:24<2:20:52, 1408.68s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 5146 samples, validate on 2206 samples\n",
            "Epoch 1/30\n",
            "5146/5146 [==============================] - 68s 13ms/step - loss: 1.3690 - acc: 0.4015 - val_loss: 1.2471 - val_acc: 0.4470\n",
            "Epoch 2/30\n",
            "5146/5146 [==============================] - 66s 13ms/step - loss: 1.2366 - acc: 0.4703 - val_loss: 1.0517 - val_acc: 0.5802\n",
            "Epoch 3/30\n",
            "5146/5146 [==============================] - 67s 13ms/step - loss: 0.8480 - acc: 0.6154 - val_loss: 0.6745 - val_acc: 0.6754\n",
            "Epoch 4/30\n",
            "5146/5146 [==============================] - 67s 13ms/step - loss: 0.6538 - acc: 0.6926 - val_loss: 0.6266 - val_acc: 0.7194\n",
            "Epoch 5/30\n",
            "5146/5146 [==============================] - 68s 13ms/step - loss: 0.5704 - acc: 0.7588 - val_loss: 1.4384 - val_acc: 0.3935\n",
            "Epoch 6/30\n",
            "5146/5146 [==============================] - 66s 13ms/step - loss: 1.4551 - acc: 0.3721 - val_loss: 1.3497 - val_acc: 0.4878\n",
            "Epoch 7/30\n",
            "5146/5146 [==============================] - 65s 13ms/step - loss: 1.1996 - acc: 0.4769 - val_loss: 0.9654 - val_acc: 0.5925\n",
            "Epoch 8/30\n",
            "5146/5146 [==============================] - 65s 13ms/step - loss: 1.3518 - acc: 0.4153 - val_loss: 1.0370 - val_acc: 0.5743\n",
            "Epoch 9/30\n",
            "5146/5146 [==============================] - 64s 12ms/step - loss: 0.9649 - acc: 0.5659 - val_loss: 0.7324 - val_acc: 0.6609\n",
            "Epoch 10/30\n",
            "5146/5146 [==============================] - 65s 13ms/step - loss: 0.7385 - acc: 0.6809 - val_loss: 0.8742 - val_acc: 0.6700\n",
            "Epoch 11/30\n",
            "5146/5146 [==============================] - 63s 12ms/step - loss: 0.7142 - acc: 0.7149 - val_loss: 0.5862 - val_acc: 0.7384\n",
            "Epoch 12/30\n",
            "5146/5146 [==============================] - 65s 13ms/step - loss: 0.5745 - acc: 0.7550 - val_loss: 0.4914 - val_acc: 0.8114\n",
            "Epoch 13/30\n",
            "5146/5146 [==============================] - 63s 12ms/step - loss: 0.4686 - acc: 0.8133 - val_loss: 0.3715 - val_acc: 0.8522\n",
            "Epoch 14/30\n",
            "5146/5146 [==============================] - 64s 12ms/step - loss: 0.3500 - acc: 0.8737 - val_loss: 0.3423 - val_acc: 0.8790\n",
            "Epoch 15/30\n",
            "5146/5146 [==============================] - 63s 12ms/step - loss: 0.3028 - acc: 0.8976 - val_loss: 0.2579 - val_acc: 0.9198\n",
            "Epoch 16/30\n",
            "5146/5146 [==============================] - 63s 12ms/step - loss: 0.2548 - acc: 0.9127 - val_loss: 0.1921 - val_acc: 0.9383\n",
            "Epoch 17/30\n",
            "5146/5146 [==============================] - 63s 12ms/step - loss: 0.1947 - acc: 0.9302 - val_loss: 0.1980 - val_acc: 0.9243\n",
            "Epoch 18/30\n",
            "5146/5146 [==============================] - 63s 12ms/step - loss: 0.3559 - acc: 0.8613 - val_loss: 0.2332 - val_acc: 0.9034\n",
            "Epoch 19/30\n",
            "5146/5146 [==============================] - 63s 12ms/step - loss: 0.2066 - acc: 0.9295 - val_loss: 0.1643 - val_acc: 0.9470\n",
            "Epoch 20/30\n",
            "5146/5146 [==============================] - 63s 12ms/step - loss: 0.1967 - acc: 0.9271 - val_loss: 0.1762 - val_acc: 0.9361\n",
            "Epoch 21/30\n",
            "5146/5146 [==============================] - 62s 12ms/step - loss: 0.1699 - acc: 0.9357 - val_loss: 0.1679 - val_acc: 0.9388\n",
            "Epoch 22/30\n",
            "5146/5146 [==============================] - 63s 12ms/step - loss: 0.1750 - acc: 0.9370 - val_loss: 0.1496 - val_acc: 0.9383\n",
            "Epoch 23/30\n",
            "5146/5146 [==============================] - 62s 12ms/step - loss: 0.1425 - acc: 0.9477 - val_loss: 0.1272 - val_acc: 0.9492\n",
            "Epoch 24/30\n",
            "5146/5146 [==============================] - 63s 12ms/step - loss: 0.1596 - acc: 0.9403 - val_loss: 0.2153 - val_acc: 0.9320\n",
            "Epoch 25/30\n",
            "5146/5146 [==============================] - 62s 12ms/step - loss: 0.1470 - acc: 0.9466 - val_loss: 0.1380 - val_acc: 0.9424\n",
            "Epoch 26/30\n",
            "5146/5146 [==============================] - 62s 12ms/step - loss: 0.1254 - acc: 0.9526 - val_loss: 0.1155 - val_acc: 0.9606\n",
            "Epoch 27/30\n",
            "5146/5146 [==============================] - 63s 12ms/step - loss: 0.1215 - acc: 0.9541 - val_loss: 0.1786 - val_acc: 0.9343\n",
            "Epoch 28/30\n",
            "5146/5146 [==============================] - 62s 12ms/step - loss: 0.1604 - acc: 0.9384 - val_loss: 0.1322 - val_acc: 0.9556\n",
            "Epoch 29/30\n",
            "5146/5146 [==============================] - 62s 12ms/step - loss: 0.1324 - acc: 0.9452 - val_loss: 0.1226 - val_acc: 0.9479\n",
            "Epoch 30/30\n",
            "5146/5146 [==============================] - 62s 12ms/step - loss: 0.1280 - acc: 0.9473 - val_loss: 0.1251 - val_acc: 0.9415\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 44%|████▍     | 4/9 [1:42:23<2:10:10, 1562.02s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 5146 samples, validate on 2206 samples\n",
            "Epoch 1/30\n",
            "5146/5146 [==============================] - 40s 8ms/step - loss: 1.2937 - acc: 0.4530 - val_loss: 1.1076 - val_acc: 0.5345\n",
            "Epoch 2/30\n",
            "5146/5146 [==============================] - 40s 8ms/step - loss: 0.9717 - acc: 0.5556 - val_loss: 0.9588 - val_acc: 0.5063\n",
            "Epoch 3/30\n",
            "5146/5146 [==============================] - 39s 8ms/step - loss: 0.7649 - acc: 0.6343 - val_loss: 0.7068 - val_acc: 0.6682\n",
            "Epoch 4/30\n",
            "5146/5146 [==============================] - 39s 8ms/step - loss: 0.6890 - acc: 0.6751 - val_loss: 0.6599 - val_acc: 0.6895\n",
            "Epoch 5/30\n",
            "5146/5146 [==============================] - 39s 8ms/step - loss: 0.6562 - acc: 0.7023 - val_loss: 0.6252 - val_acc: 0.7212\n",
            "Epoch 6/30\n",
            "5146/5146 [==============================] - 39s 8ms/step - loss: 0.6369 - acc: 0.6982 - val_loss: 0.5892 - val_acc: 0.7153\n",
            "Epoch 7/30\n",
            "5146/5146 [==============================] - 39s 8ms/step - loss: 0.6847 - acc: 0.6906 - val_loss: 0.6519 - val_acc: 0.6999\n",
            "Epoch 8/30\n",
            "5146/5146 [==============================] - 39s 8ms/step - loss: 0.6355 - acc: 0.7165 - val_loss: 0.4838 - val_acc: 0.7770\n",
            "Epoch 9/30\n",
            "5146/5146 [==============================] - 39s 8ms/step - loss: 0.4739 - acc: 0.7921 - val_loss: 0.3971 - val_acc: 0.8463\n",
            "Epoch 10/30\n",
            "5146/5146 [==============================] - 39s 8ms/step - loss: 0.3824 - acc: 0.8465 - val_loss: 0.3364 - val_acc: 0.8844\n",
            "Epoch 11/30\n",
            "5146/5146 [==============================] - 39s 8ms/step - loss: 0.3499 - acc: 0.8673 - val_loss: 0.2883 - val_acc: 0.9098\n",
            "Epoch 12/30\n",
            "5146/5146 [==============================] - 39s 8ms/step - loss: 0.2707 - acc: 0.9081 - val_loss: 0.2498 - val_acc: 0.9130\n",
            "Epoch 13/30\n",
            "5146/5146 [==============================] - 39s 8ms/step - loss: 0.2413 - acc: 0.9170 - val_loss: 0.2544 - val_acc: 0.9152\n",
            "Epoch 14/30\n",
            "5146/5146 [==============================] - 39s 8ms/step - loss: 0.2130 - acc: 0.9271 - val_loss: 0.2176 - val_acc: 0.9248\n",
            "Epoch 15/30\n",
            "5146/5146 [==============================] - 39s 8ms/step - loss: 0.1930 - acc: 0.9302 - val_loss: 0.1991 - val_acc: 0.9252\n",
            "Epoch 16/30\n",
            "5146/5146 [==============================] - 39s 8ms/step - loss: 0.1910 - acc: 0.9322 - val_loss: 0.2142 - val_acc: 0.9270\n",
            "Epoch 17/30\n",
            "5146/5146 [==============================] - 39s 8ms/step - loss: 0.2318 - acc: 0.9174 - val_loss: 0.2858 - val_acc: 0.8681\n",
            "Epoch 18/30\n",
            "5146/5146 [==============================] - 40s 8ms/step - loss: 0.3193 - acc: 0.8780 - val_loss: 0.2738 - val_acc: 0.9039\n",
            "Epoch 19/30\n",
            "5146/5146 [==============================] - 39s 8ms/step - loss: 0.2455 - acc: 0.9040 - val_loss: 0.2457 - val_acc: 0.9039\n",
            "Epoch 20/30\n",
            "5146/5146 [==============================] - 39s 8ms/step - loss: 0.1861 - acc: 0.9339 - val_loss: 0.1981 - val_acc: 0.9429\n",
            "Epoch 21/30\n",
            "5146/5146 [==============================] - 39s 8ms/step - loss: 0.1714 - acc: 0.9355 - val_loss: 0.1680 - val_acc: 0.9383\n",
            "Epoch 22/30\n",
            "5146/5146 [==============================] - 39s 8ms/step - loss: 0.1737 - acc: 0.9376 - val_loss: 0.1663 - val_acc: 0.9429\n",
            "Epoch 23/30\n",
            "5146/5146 [==============================] - 39s 8ms/step - loss: 0.1610 - acc: 0.9376 - val_loss: 0.1729 - val_acc: 0.9415\n",
            "Epoch 24/30\n",
            "5146/5146 [==============================] - 40s 8ms/step - loss: 0.1402 - acc: 0.9466 - val_loss: 0.1582 - val_acc: 0.9515\n",
            "Epoch 25/30\n",
            "5146/5146 [==============================] - 41s 8ms/step - loss: 0.1370 - acc: 0.9425 - val_loss: 0.2544 - val_acc: 0.9034\n",
            "Epoch 26/30\n",
            "5146/5146 [==============================] - 42s 8ms/step - loss: 0.1374 - acc: 0.9487 - val_loss: 0.1496 - val_acc: 0.9438\n",
            "Epoch 27/30\n",
            "5146/5146 [==============================] - 41s 8ms/step - loss: 0.1312 - acc: 0.9487 - val_loss: 0.1550 - val_acc: 0.9470\n",
            "Epoch 28/30\n",
            "5146/5146 [==============================] - 41s 8ms/step - loss: 0.1769 - acc: 0.9333 - val_loss: 0.2148 - val_acc: 0.9270\n",
            "Epoch 29/30\n",
            "5146/5146 [==============================] - 40s 8ms/step - loss: 0.2784 - acc: 0.8618 - val_loss: 0.3044 - val_acc: 0.8286\n",
            "Epoch 30/30\n",
            "5146/5146 [==============================] - 41s 8ms/step - loss: 0.2652 - acc: 0.8644 - val_loss: 0.1924 - val_acc: 0.9320\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 56%|█████▌    | 5/9 [2:02:12<1:36:39, 1449.84s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 5146 samples, validate on 2206 samples\n",
            "Epoch 1/30\n",
            "5146/5146 [==============================] - 69s 13ms/step - loss: 1.3953 - acc: 0.3976 - val_loss: 1.3560 - val_acc: 0.3799\n",
            "Epoch 2/30\n",
            "5146/5146 [==============================] - 68s 13ms/step - loss: 1.3477 - acc: 0.3782 - val_loss: 1.3695 - val_acc: 0.3767\n",
            "Epoch 3/30\n",
            "5146/5146 [==============================] - 66s 13ms/step - loss: 1.3742 - acc: 0.3690 - val_loss: 1.3900 - val_acc: 0.3223\n",
            "Epoch 4/30\n",
            "5146/5146 [==============================] - 68s 13ms/step - loss: 1.3390 - acc: 0.4118 - val_loss: 1.3737 - val_acc: 0.4519\n",
            "Epoch 5/30\n",
            "5146/5146 [==============================] - 67s 13ms/step - loss: 1.0628 - acc: 0.4800 - val_loss: 1.7544 - val_acc: 0.3382\n",
            "Epoch 6/30\n",
            "5146/5146 [==============================] - 66s 13ms/step - loss: 1.0266 - acc: 0.5321 - val_loss: 0.8038 - val_acc: 0.6278\n",
            "Epoch 7/30\n",
            "5146/5146 [==============================] - 65s 13ms/step - loss: 0.7592 - acc: 0.6288 - val_loss: 0.6997 - val_acc: 0.6487\n",
            "Epoch 8/30\n",
            "5146/5146 [==============================] - 64s 12ms/step - loss: 0.7183 - acc: 0.6370 - val_loss: 0.6646 - val_acc: 0.6546\n",
            "Epoch 9/30\n",
            "5146/5146 [==============================] - 64s 12ms/step - loss: 0.7735 - acc: 0.6127 - val_loss: 0.6415 - val_acc: 0.6573\n",
            "Epoch 10/30\n",
            "5146/5146 [==============================] - 64s 12ms/step - loss: 0.6489 - acc: 0.6726 - val_loss: 0.6023 - val_acc: 0.7212\n",
            "Epoch 11/30\n",
            "5146/5146 [==============================] - 64s 12ms/step - loss: 0.6222 - acc: 0.7132 - val_loss: 0.6962 - val_acc: 0.6895\n",
            "Epoch 12/30\n",
            "5146/5146 [==============================] - 65s 13ms/step - loss: 0.5559 - acc: 0.7639 - val_loss: 0.5377 - val_acc: 0.7665\n",
            "Epoch 13/30\n",
            "5146/5146 [==============================] - 64s 12ms/step - loss: 0.4770 - acc: 0.7962 - val_loss: 0.5061 - val_acc: 0.7593\n",
            "Epoch 14/30\n",
            "5146/5146 [==============================] - 64s 12ms/step - loss: 0.4440 - acc: 0.8119 - val_loss: 0.4034 - val_acc: 0.8218\n",
            "Epoch 15/30\n",
            "5146/5146 [==============================] - 63s 12ms/step - loss: 0.3633 - acc: 0.8644 - val_loss: 0.5003 - val_acc: 0.7910\n",
            "Epoch 16/30\n",
            "5146/5146 [==============================] - 63s 12ms/step - loss: 0.2762 - acc: 0.9017 - val_loss: 0.2397 - val_acc: 0.9284\n",
            "Epoch 17/30\n",
            "5146/5146 [==============================] - 64s 13ms/step - loss: 0.2396 - acc: 0.9178 - val_loss: 0.1987 - val_acc: 0.9361\n",
            "Epoch 18/30\n",
            "5146/5146 [==============================] - 64s 12ms/step - loss: 0.1670 - acc: 0.9409 - val_loss: 0.1783 - val_acc: 0.9438\n",
            "Epoch 19/30\n",
            "5146/5146 [==============================] - 64s 12ms/step - loss: 0.1581 - acc: 0.9427 - val_loss: 0.1846 - val_acc: 0.9334\n",
            "Epoch 20/30\n",
            "5146/5146 [==============================] - 64s 12ms/step - loss: 0.1548 - acc: 0.9442 - val_loss: 0.1899 - val_acc: 0.9393\n",
            "Epoch 21/30\n",
            "5146/5146 [==============================] - 63s 12ms/step - loss: 0.1332 - acc: 0.9479 - val_loss: 0.1254 - val_acc: 0.9538\n",
            "Epoch 22/30\n",
            "5146/5146 [==============================] - 65s 13ms/step - loss: 0.1224 - acc: 0.9518 - val_loss: 0.1645 - val_acc: 0.9442\n",
            "Epoch 23/30\n",
            "5146/5146 [==============================] - 65s 13ms/step - loss: 0.1610 - acc: 0.9324 - val_loss: 0.1482 - val_acc: 0.9356\n",
            "Epoch 24/30\n",
            "5146/5146 [==============================] - 68s 13ms/step - loss: 0.1536 - acc: 0.9409 - val_loss: 0.4095 - val_acc: 0.8808\n",
            "Epoch 25/30\n",
            "5146/5146 [==============================] - 66s 13ms/step - loss: 0.1781 - acc: 0.9405 - val_loss: 0.2094 - val_acc: 0.9170\n",
            "Epoch 26/30\n",
            "5146/5146 [==============================] - 67s 13ms/step - loss: 0.1749 - acc: 0.9310 - val_loss: 0.2050 - val_acc: 0.9184\n",
            "Epoch 27/30\n",
            "5146/5146 [==============================] - 68s 13ms/step - loss: 0.2084 - acc: 0.9126 - val_loss: 0.1506 - val_acc: 0.9456\n",
            "Epoch 28/30\n",
            "5146/5146 [==============================] - 67s 13ms/step - loss: 0.1673 - acc: 0.9328 - val_loss: 0.3245 - val_acc: 0.8087\n",
            "Epoch 29/30\n",
            "5146/5146 [==============================] - 66s 13ms/step - loss: 0.1586 - acc: 0.9347 - val_loss: 0.1500 - val_acc: 0.9311\n",
            "Epoch 30/30\n",
            "5146/5146 [==============================] - 66s 13ms/step - loss: 0.1293 - acc: 0.9504 - val_loss: 0.1178 - val_acc: 0.9551\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 67%|██████▋   | 6/9 [2:34:56<1:20:12, 1604.07s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 5146 samples, validate on 2206 samples\n",
            "Epoch 1/30\n",
            "5146/5146 [==============================] - 45s 9ms/step - loss: 1.3352 - acc: 0.4330 - val_loss: 0.9891 - val_acc: 0.5172\n",
            "Epoch 2/30\n",
            "5146/5146 [==============================] - 42s 8ms/step - loss: 0.9922 - acc: 0.5468 - val_loss: 0.9177 - val_acc: 0.6124\n",
            "Epoch 3/30\n",
            "5146/5146 [==============================] - 41s 8ms/step - loss: 0.7860 - acc: 0.6459 - val_loss: 0.7048 - val_acc: 0.6510\n",
            "Epoch 4/30\n",
            "5146/5146 [==============================] - 41s 8ms/step - loss: 0.7723 - acc: 0.6387 - val_loss: 0.7154 - val_acc: 0.6813\n",
            "Epoch 5/30\n",
            "5146/5146 [==============================] - 42s 8ms/step - loss: 0.7157 - acc: 0.6681 - val_loss: 0.6649 - val_acc: 0.6913\n",
            "Epoch 6/30\n",
            "5146/5146 [==============================] - 42s 8ms/step - loss: 0.8827 - acc: 0.5680 - val_loss: 0.7588 - val_acc: 0.6024\n",
            "Epoch 7/30\n",
            "5146/5146 [==============================] - 42s 8ms/step - loss: 0.7230 - acc: 0.6611 - val_loss: 0.6727 - val_acc: 0.6392\n",
            "Epoch 8/30\n",
            "5146/5146 [==============================] - 41s 8ms/step - loss: 0.6230 - acc: 0.7095 - val_loss: 0.5518 - val_acc: 0.7375\n",
            "Epoch 9/30\n",
            "5146/5146 [==============================] - 41s 8ms/step - loss: 0.6728 - acc: 0.6494 - val_loss: 0.5888 - val_acc: 0.7321\n",
            "Epoch 10/30\n",
            "5146/5146 [==============================] - 42s 8ms/step - loss: 0.5092 - acc: 0.7483 - val_loss: 0.4564 - val_acc: 0.7743\n",
            "Epoch 11/30\n",
            "5146/5146 [==============================] - 41s 8ms/step - loss: 0.4247 - acc: 0.8031 - val_loss: 0.4311 - val_acc: 0.7915\n",
            "Epoch 12/30\n",
            "5146/5146 [==============================] - 41s 8ms/step - loss: 0.4258 - acc: 0.8294 - val_loss: 0.4135 - val_acc: 0.8105\n",
            "Epoch 13/30\n",
            "5146/5146 [==============================] - 42s 8ms/step - loss: 0.3360 - acc: 0.8809 - val_loss: 0.3901 - val_acc: 0.8527\n",
            "Epoch 14/30\n",
            "5146/5146 [==============================] - 41s 8ms/step - loss: 0.3026 - acc: 0.8908 - val_loss: 0.2925 - val_acc: 0.8948\n",
            "Epoch 15/30\n",
            "5146/5146 [==============================] - 42s 8ms/step - loss: 0.2368 - acc: 0.9143 - val_loss: 0.2259 - val_acc: 0.9225\n",
            "Epoch 16/30\n",
            "5146/5146 [==============================] - 43s 8ms/step - loss: 0.2246 - acc: 0.9217 - val_loss: 0.2060 - val_acc: 0.9297\n",
            "Epoch 17/30\n",
            "5146/5146 [==============================] - 42s 8ms/step - loss: 0.2177 - acc: 0.9188 - val_loss: 0.2346 - val_acc: 0.9121\n",
            "Epoch 18/30\n",
            "5146/5146 [==============================] - 42s 8ms/step - loss: 0.1861 - acc: 0.9361 - val_loss: 0.1743 - val_acc: 0.9374\n",
            "Epoch 19/30\n",
            "5146/5146 [==============================] - 41s 8ms/step - loss: 0.1769 - acc: 0.9335 - val_loss: 0.1655 - val_acc: 0.9361\n",
            "Epoch 20/30\n",
            "5146/5146 [==============================] - 42s 8ms/step - loss: 0.1534 - acc: 0.9442 - val_loss: 0.1711 - val_acc: 0.9365\n",
            "Epoch 21/30\n",
            "5146/5146 [==============================] - 41s 8ms/step - loss: 0.2414 - acc: 0.9182 - val_loss: 0.3340 - val_acc: 0.9034\n",
            "Epoch 22/30\n",
            "5146/5146 [==============================] - 41s 8ms/step - loss: 0.1770 - acc: 0.9355 - val_loss: 0.1801 - val_acc: 0.9361\n",
            "Epoch 23/30\n",
            "5146/5146 [==============================] - 42s 8ms/step - loss: 0.1618 - acc: 0.9405 - val_loss: 0.1626 - val_acc: 0.9383\n",
            "Epoch 24/30\n",
            "5146/5146 [==============================] - 41s 8ms/step - loss: 0.1708 - acc: 0.9417 - val_loss: 0.1664 - val_acc: 0.9429\n",
            "Epoch 25/30\n",
            "5146/5146 [==============================] - 40s 8ms/step - loss: 0.1423 - acc: 0.9440 - val_loss: 0.1503 - val_acc: 0.9402\n",
            "Epoch 26/30\n",
            "5146/5146 [==============================] - 40s 8ms/step - loss: 0.1387 - acc: 0.9473 - val_loss: 0.1338 - val_acc: 0.9510\n",
            "Epoch 27/30\n",
            "5146/5146 [==============================] - 40s 8ms/step - loss: 0.1350 - acc: 0.9499 - val_loss: 0.1432 - val_acc: 0.9383\n",
            "Epoch 28/30\n",
            "5146/5146 [==============================] - 40s 8ms/step - loss: 0.1234 - acc: 0.9493 - val_loss: 0.1388 - val_acc: 0.9519\n",
            "Epoch 29/30\n",
            "5146/5146 [==============================] - 40s 8ms/step - loss: 0.1334 - acc: 0.9493 - val_loss: 0.1437 - val_acc: 0.9483\n",
            "Epoch 30/30\n",
            "5146/5146 [==============================] - 40s 8ms/step - loss: 0.1224 - acc: 0.9508 - val_loss: 0.1239 - val_acc: 0.9442\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 78%|███████▊  | 7/9 [2:55:38<49:51, 1495.74s/it]  \u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 5146 samples, validate on 2206 samples\n",
            "Epoch 1/30\n",
            "5146/5146 [==============================] - 42s 8ms/step - loss: 1.1741 - acc: 0.5317 - val_loss: 0.7982 - val_acc: 0.6047\n",
            "Epoch 2/30\n",
            "5146/5146 [==============================] - 40s 8ms/step - loss: 0.7257 - acc: 0.6881 - val_loss: 0.6044 - val_acc: 0.7489\n",
            "Epoch 3/30\n",
            "5146/5146 [==============================] - 40s 8ms/step - loss: 0.5968 - acc: 0.7476 - val_loss: 0.4850 - val_acc: 0.7820\n",
            "Epoch 4/30\n",
            "5146/5146 [==============================] - 40s 8ms/step - loss: 0.7059 - acc: 0.7044 - val_loss: 0.6295 - val_acc: 0.7906\n",
            "Epoch 5/30\n",
            "5146/5146 [==============================] - 40s 8ms/step - loss: 0.5539 - acc: 0.7800 - val_loss: 0.4386 - val_acc: 0.8432\n",
            "Epoch 6/30\n",
            "5146/5146 [==============================] - 40s 8ms/step - loss: 0.4268 - acc: 0.8405 - val_loss: 0.3469 - val_acc: 0.8808\n",
            "Epoch 7/30\n",
            "5146/5146 [==============================] - 40s 8ms/step - loss: 0.4059 - acc: 0.8545 - val_loss: 0.4293 - val_acc: 0.8391\n",
            "Epoch 8/30\n",
            "5146/5146 [==============================] - 40s 8ms/step - loss: 0.2940 - acc: 0.8995 - val_loss: 0.2975 - val_acc: 0.9016\n",
            "Epoch 9/30\n",
            "5146/5146 [==============================] - 40s 8ms/step - loss: 0.2494 - acc: 0.9159 - val_loss: 0.2342 - val_acc: 0.9234\n",
            "Epoch 10/30\n",
            "5146/5146 [==============================] - 39s 8ms/step - loss: 0.2554 - acc: 0.9151 - val_loss: 0.2310 - val_acc: 0.9275\n",
            "Epoch 11/30\n",
            "5146/5146 [==============================] - 40s 8ms/step - loss: 0.2227 - acc: 0.9248 - val_loss: 0.2296 - val_acc: 0.9293\n",
            "Epoch 12/30\n",
            "5146/5146 [==============================] - 40s 8ms/step - loss: 0.2053 - acc: 0.9275 - val_loss: 0.2104 - val_acc: 0.9334\n",
            "Epoch 13/30\n",
            "5146/5146 [==============================] - 40s 8ms/step - loss: 0.2127 - acc: 0.9254 - val_loss: 0.2209 - val_acc: 0.9189\n",
            "Epoch 14/30\n",
            "5146/5146 [==============================] - 40s 8ms/step - loss: 0.2810 - acc: 0.8991 - val_loss: 0.4484 - val_acc: 0.8509\n",
            "Epoch 15/30\n",
            "5146/5146 [==============================] - 40s 8ms/step - loss: 0.2627 - acc: 0.9048 - val_loss: 0.2124 - val_acc: 0.9225\n",
            "Epoch 16/30\n",
            "5146/5146 [==============================] - 40s 8ms/step - loss: 0.2058 - acc: 0.9246 - val_loss: 0.2160 - val_acc: 0.9316\n",
            "Epoch 17/30\n",
            "5146/5146 [==============================] - 40s 8ms/step - loss: 0.1832 - acc: 0.9293 - val_loss: 0.1709 - val_acc: 0.9329\n",
            "Epoch 18/30\n",
            "5146/5146 [==============================] - 39s 8ms/step - loss: 0.1751 - acc: 0.9320 - val_loss: 0.1598 - val_acc: 0.9338\n",
            "Epoch 19/30\n",
            "5146/5146 [==============================] - 40s 8ms/step - loss: 0.1475 - acc: 0.9376 - val_loss: 0.1472 - val_acc: 0.9365\n",
            "Epoch 20/30\n",
            "5146/5146 [==============================] - 40s 8ms/step - loss: 0.1613 - acc: 0.9370 - val_loss: 0.1537 - val_acc: 0.9483\n",
            "Epoch 21/30\n",
            "5146/5146 [==============================] - 40s 8ms/step - loss: 0.1881 - acc: 0.9318 - val_loss: 0.2362 - val_acc: 0.9053\n",
            "Epoch 22/30\n",
            "5146/5146 [==============================] - 40s 8ms/step - loss: 0.1471 - acc: 0.9448 - val_loss: 0.1670 - val_acc: 0.9293\n",
            "Epoch 23/30\n",
            "5146/5146 [==============================] - 40s 8ms/step - loss: 0.1428 - acc: 0.9456 - val_loss: 0.1272 - val_acc: 0.9515\n",
            "Epoch 24/30\n",
            "5146/5146 [==============================] - 39s 8ms/step - loss: 0.1419 - acc: 0.9446 - val_loss: 0.1477 - val_acc: 0.9488\n",
            "Epoch 25/30\n",
            "5146/5146 [==============================] - 40s 8ms/step - loss: 0.1247 - acc: 0.9518 - val_loss: 0.1308 - val_acc: 0.9533\n",
            "Epoch 26/30\n",
            "5146/5146 [==============================] - 39s 8ms/step - loss: 0.1411 - acc: 0.9419 - val_loss: 0.1258 - val_acc: 0.9533\n",
            "Epoch 27/30\n",
            "5146/5146 [==============================] - 39s 8ms/step - loss: 0.1328 - acc: 0.9450 - val_loss: 0.1253 - val_acc: 0.9547\n",
            "Epoch 28/30\n",
            "5146/5146 [==============================] - 40s 8ms/step - loss: 0.1356 - acc: 0.9475 - val_loss: 0.1470 - val_acc: 0.9510\n",
            "Epoch 29/30\n",
            "5146/5146 [==============================] - 40s 8ms/step - loss: 0.1503 - acc: 0.9442 - val_loss: 0.1473 - val_acc: 0.9433\n",
            "Epoch 30/30\n",
            "5146/5146 [==============================] - 39s 8ms/step - loss: 0.1284 - acc: 0.9503 - val_loss: 0.1211 - val_acc: 0.9551\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 89%|████████▉ | 8/9 [3:15:35<23:26, 1406.11s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 5146 samples, validate on 2206 samples\n",
            "Epoch 1/30\n",
            "5146/5146 [==============================] - 66s 13ms/step - loss: 1.4298 - acc: 0.3508 - val_loss: 1.3359 - val_acc: 0.3817\n",
            "Epoch 2/30\n",
            "5146/5146 [==============================] - 64s 12ms/step - loss: 1.5386 - acc: 0.3210 - val_loss: 1.5670 - val_acc: 0.3001\n",
            "Epoch 3/30\n",
            "5146/5146 [==============================] - 64s 12ms/step - loss: 1.4224 - acc: 0.3708 - val_loss: 1.3814 - val_acc: 0.3803\n",
            "Epoch 4/30\n",
            "5146/5146 [==============================] - 64s 12ms/step - loss: 1.3699 - acc: 0.3712 - val_loss: 1.3782 - val_acc: 0.3803\n",
            "Epoch 5/30\n",
            "5146/5146 [==============================] - 64s 12ms/step - loss: 1.3672 - acc: 0.3684 - val_loss: 1.3620 - val_acc: 0.3803\n",
            "Epoch 6/30\n",
            "5146/5146 [==============================] - 63s 12ms/step - loss: 1.3565 - acc: 0.3661 - val_loss: 1.3733 - val_acc: 0.3803\n",
            "Epoch 7/30\n",
            "5146/5146 [==============================] - 64s 12ms/step - loss: 1.3176 - acc: 0.3991 - val_loss: 1.2840 - val_acc: 0.4415\n",
            "Epoch 8/30\n",
            "5146/5146 [==============================] - 63s 12ms/step - loss: 1.3479 - acc: 0.3712 - val_loss: 1.3312 - val_acc: 0.3817\n",
            "Epoch 9/30\n",
            "5146/5146 [==============================] - 63s 12ms/step - loss: 1.3265 - acc: 0.3927 - val_loss: 1.3362 - val_acc: 0.3572\n",
            "Epoch 10/30\n",
            "5146/5146 [==============================] - 63s 12ms/step - loss: 1.3359 - acc: 0.3677 - val_loss: 1.3370 - val_acc: 0.3572\n",
            "Epoch 11/30\n",
            "5146/5146 [==============================] - 63s 12ms/step - loss: 1.3334 - acc: 0.3766 - val_loss: 1.3351 - val_acc: 0.3817\n",
            "Epoch 12/30\n",
            "5146/5146 [==============================] - 63s 12ms/step - loss: 1.3317 - acc: 0.3727 - val_loss: 1.3260 - val_acc: 0.3817\n",
            "Epoch 13/30\n",
            "5146/5146 [==============================] - 63s 12ms/step - loss: 1.3287 - acc: 0.3883 - val_loss: 1.2840 - val_acc: 0.4207\n",
            "Epoch 14/30\n",
            "5146/5146 [==============================] - 63s 12ms/step - loss: 1.0715 - acc: 0.4949 - val_loss: 0.8447 - val_acc: 0.5671\n",
            "Epoch 15/30\n",
            "5146/5146 [==============================] - 63s 12ms/step - loss: 0.9119 - acc: 0.5262 - val_loss: 0.8648 - val_acc: 0.5331\n",
            "Epoch 16/30\n",
            "5146/5146 [==============================] - 63s 12ms/step - loss: 1.0391 - acc: 0.4936 - val_loss: 0.8702 - val_acc: 0.5258\n",
            "Epoch 17/30\n",
            "5146/5146 [==============================] - 63s 12ms/step - loss: 0.7924 - acc: 0.5851 - val_loss: 0.8238 - val_acc: 0.5403\n",
            "Epoch 18/30\n",
            "5146/5146 [==============================] - 63s 12ms/step - loss: 0.7537 - acc: 0.5874 - val_loss: 0.7578 - val_acc: 0.6142\n",
            "Epoch 19/30\n",
            "5146/5146 [==============================] - 63s 12ms/step - loss: 0.7394 - acc: 0.6207 - val_loss: 0.7159 - val_acc: 0.6274\n",
            "Epoch 20/30\n",
            "5146/5146 [==============================] - 63s 12ms/step - loss: 0.7015 - acc: 0.6308 - val_loss: 0.6882 - val_acc: 0.6319\n",
            "Epoch 21/30\n",
            "5146/5146 [==============================] - 63s 12ms/step - loss: 0.6698 - acc: 0.6384 - val_loss: 0.7538 - val_acc: 0.6301\n",
            "Epoch 22/30\n",
            "5146/5146 [==============================] - 63s 12ms/step - loss: 0.8403 - acc: 0.5787 - val_loss: 0.7379 - val_acc: 0.6020\n",
            "Epoch 23/30\n",
            "5146/5146 [==============================] - 63s 12ms/step - loss: 0.7057 - acc: 0.6228 - val_loss: 0.6007 - val_acc: 0.6646\n",
            "Epoch 24/30\n",
            "5146/5146 [==============================] - 63s 12ms/step - loss: 0.5764 - acc: 0.6481 - val_loss: 0.5389 - val_acc: 0.6627\n",
            "Epoch 25/30\n",
            "5146/5146 [==============================] - 63s 12ms/step - loss: 0.5596 - acc: 0.6432 - val_loss: 0.5295 - val_acc: 0.6591\n",
            "Epoch 26/30\n",
            "5146/5146 [==============================] - 63s 12ms/step - loss: 0.5310 - acc: 0.6514 - val_loss: 0.4959 - val_acc: 0.6636\n",
            "Epoch 27/30\n",
            "5146/5146 [==============================] - 63s 12ms/step - loss: 0.5256 - acc: 0.6547 - val_loss: 0.5309 - val_acc: 0.6537\n",
            "Epoch 28/30\n",
            "5146/5146 [==============================] - 62s 12ms/step - loss: 0.4894 - acc: 0.6580 - val_loss: 0.4802 - val_acc: 0.6659\n",
            "Epoch 29/30\n",
            "5146/5146 [==============================] - 63s 12ms/step - loss: 0.4840 - acc: 0.6605 - val_loss: 0.4780 - val_acc: 0.6646\n",
            "Epoch 30/30\n",
            "5146/5146 [==============================] - 63s 12ms/step - loss: 0.4730 - acc: 0.6578 - val_loss: 0.4773 - val_acc: 0.6627\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 9/9 [3:47:13<00:00, 1553.47s/it]\u001b[A\n",
            "\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "G_H-bx1Z0lm1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Results of HyperParameter Tuning"
      ]
    },
    {
      "metadata": {
        "id": "AoHBZN1BB5jG",
        "colab_type": "code",
        "outputId": "2fd5047e-d524-4055-c603-4d8436d08b7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "cell_type": "code",
      "source": [
        "scan_object.data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>round_epochs</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_acc</th>\n",
              "      <th>loss</th>\n",
              "      <th>acc</th>\n",
              "      <th>units</th>\n",
              "      <th>Dropout</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>30</td>\n",
              "      <td>0.12650565271445652</td>\n",
              "      <td>0.9555757026291931</td>\n",
              "      <td>0.11647951732083049</td>\n",
              "      <td>0.954916439907031</td>\n",
              "      <td>64</td>\n",
              "      <td>0.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>30</td>\n",
              "      <td>0.1118909985583583</td>\n",
              "      <td>0.9605621033544878</td>\n",
              "      <td>0.1177000504572001</td>\n",
              "      <td>0.9553050913330743</td>\n",
              "      <td>64</td>\n",
              "      <td>0.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>30</td>\n",
              "      <td>0.11668117210971476</td>\n",
              "      <td>0.9555757024670771</td>\n",
              "      <td>0.11718041456155726</td>\n",
              "      <td>0.9525845316750875</td>\n",
              "      <td>64</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>30</td>\n",
              "      <td>0.11551105764002986</td>\n",
              "      <td>0.9605621033544878</td>\n",
              "      <td>0.12152238045534965</td>\n",
              "      <td>0.9541391371476062</td>\n",
              "      <td>128</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>30</td>\n",
              "      <td>0.1496263371235062</td>\n",
              "      <td>0.9514959202175884</td>\n",
              "      <td>0.13124724172904156</td>\n",
              "      <td>0.9486980178779635</td>\n",
              "      <td>32</td>\n",
              "      <td>0.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>30</td>\n",
              "      <td>0.1178307432144313</td>\n",
              "      <td>0.9551223934723482</td>\n",
              "      <td>0.12238699158062175</td>\n",
              "      <td>0.9518072289156626</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>30</td>\n",
              "      <td>0.12393545845383314</td>\n",
              "      <td>0.9519492293744334</td>\n",
              "      <td>0.12244204704979492</td>\n",
              "      <td>0.9508356004200508</td>\n",
              "      <td>32</td>\n",
              "      <td>0.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>30</td>\n",
              "      <td>0.12114448429288803</td>\n",
              "      <td>0.9551223934723482</td>\n",
              "      <td>0.12467113660783838</td>\n",
              "      <td>0.9518072289156626</td>\n",
              "      <td>32</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>30</td>\n",
              "      <td>0.47728870426754677</td>\n",
              "      <td>0.6659111513512197</td>\n",
              "      <td>0.4729716697880789</td>\n",
              "      <td>0.6605130198443858</td>\n",
              "      <td>128</td>\n",
              "      <td>0.8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  round_epochs             val_loss             val_acc                 loss  \\\n",
              "0           30  0.12650565271445652  0.9555757026291931  0.11647951732083049   \n",
              "1           30   0.1118909985583583  0.9605621033544878   0.1177000504572001   \n",
              "2           30  0.11668117210971476  0.9555757024670771  0.11718041456155726   \n",
              "3           30  0.11551105764002986  0.9605621033544878  0.12152238045534965   \n",
              "4           30   0.1496263371235062  0.9514959202175884  0.13124724172904156   \n",
              "5           30   0.1178307432144313  0.9551223934723482  0.12238699158062175   \n",
              "6           30  0.12393545845383314  0.9519492293744334  0.12244204704979492   \n",
              "7           30  0.12114448429288803  0.9551223934723482  0.12467113660783838   \n",
              "8           30  0.47728870426754677  0.6659111513512197   0.4729716697880789   \n",
              "\n",
              "                  acc units Dropout  \n",
              "0   0.954916439907031    64     0.8  \n",
              "1  0.9553050913330743    64     0.7  \n",
              "2  0.9525845316750875    64     0.5  \n",
              "3  0.9541391371476062   128     0.5  \n",
              "4  0.9486980178779635    32     0.7  \n",
              "5  0.9518072289156626   128     0.7  \n",
              "6  0.9508356004200508    32     0.8  \n",
              "7  0.9518072289156626    32     0.5  \n",
              "8  0.6605130198443858   128     0.8  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "wM83Rcab0sZH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Best HyperParameter values are : UNITS =64 ,Dropout=0.7,batchsize=16,epoch=30"
      ]
    },
    {
      "metadata": {
        "id": "uB3J-kHl1CBt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training Model from best HyperParameters valuse"
      ]
    },
    {
      "metadata": {
        "id": "ZMilxO39_F97",
        "colab_type": "code",
        "outputId": "d7a946e9-8279-4f3b-f6d9-3e0d16c1f129",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1071
        }
      },
      "cell_type": "code",
      "source": [
        "#best accuracy= 64 units and DRopout =0.7 and batchsizre 6 and epoch 30\n",
        "model = Sequential()\n",
        "model.add(LSTM(64,return_sequences=True, input_shape=(128,9)))\n",
        "model.add(Dropout(0.7))\n",
        "model.add(LSTM(32))\n",
        "model.add(Dense(6, activation='sigmoid'))\n",
        "\n",
        "adam=Adam(lr=1e-3)\n",
        "model.compile(optimizer=adam , loss='categorical_crossentropy' ,metrics=['accuracy'])\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          batch_size=16,\n",
        "          validation_data=(x_test, y_test),\n",
        "          epochs=30)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7352 samples, validate on 2947 samples\n",
            "Epoch 1/30\n",
            "7352/7352 [==============================] - 67s 9ms/step - loss: 1.0599 - acc: 0.5486 - val_loss: 0.8127 - val_acc: 0.6179\n",
            "Epoch 2/30\n",
            "7352/7352 [==============================] - 65s 9ms/step - loss: 0.8177 - acc: 0.6155 - val_loss: 0.8009 - val_acc: 0.6125\n",
            "Epoch 3/30\n",
            "7352/7352 [==============================] - 66s 9ms/step - loss: 0.6058 - acc: 0.7393 - val_loss: 0.6377 - val_acc: 0.6756\n",
            "Epoch 4/30\n",
            "7352/7352 [==============================] - 66s 9ms/step - loss: 0.4715 - acc: 0.7969 - val_loss: 0.4688 - val_acc: 0.8422\n",
            "Epoch 5/30\n",
            "7352/7352 [==============================] - 69s 9ms/step - loss: 0.3193 - acc: 0.8901 - val_loss: 0.3109 - val_acc: 0.9036\n",
            "Epoch 6/30\n",
            "7352/7352 [==============================] - 67s 9ms/step - loss: 0.2212 - acc: 0.9249 - val_loss: 0.3184 - val_acc: 0.9002\n",
            "Epoch 7/30\n",
            "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1845 - acc: 0.9376 - val_loss: 0.2805 - val_acc: 0.9013\n",
            "Epoch 8/30\n",
            "7352/7352 [==============================] - 66s 9ms/step - loss: 0.1866 - acc: 0.9321 - val_loss: 0.2856 - val_acc: 0.9125\n",
            "Epoch 9/30\n",
            "7352/7352 [==============================] - 66s 9ms/step - loss: 0.1773 - acc: 0.9344 - val_loss: 0.2875 - val_acc: 0.9070\n",
            "Epoch 10/30\n",
            "7352/7352 [==============================] - 66s 9ms/step - loss: 0.1290 - acc: 0.9527 - val_loss: 0.2630 - val_acc: 0.9094\n",
            "Epoch 11/30\n",
            "7352/7352 [==============================] - 66s 9ms/step - loss: 0.1286 - acc: 0.9504 - val_loss: 0.2538 - val_acc: 0.9145\n",
            "Epoch 12/30\n",
            "7352/7352 [==============================] - 67s 9ms/step - loss: 0.1273 - acc: 0.9499 - val_loss: 0.2388 - val_acc: 0.9131\n",
            "Epoch 13/30\n",
            "7352/7352 [==============================] - 66s 9ms/step - loss: 0.1374 - acc: 0.9493 - val_loss: 0.2406 - val_acc: 0.9087\n",
            "Epoch 14/30\n",
            "7352/7352 [==============================] - 67s 9ms/step - loss: 0.1439 - acc: 0.9455 - val_loss: 0.3138 - val_acc: 0.9063\n",
            "Epoch 15/30\n",
            "7352/7352 [==============================] - 140s 19ms/step - loss: 0.1131 - acc: 0.9531 - val_loss: 0.2780 - val_acc: 0.9145\n",
            "Epoch 16/30\n",
            "7352/7352 [==============================] - 105s 14ms/step - loss: 0.1159 - acc: 0.9532 - val_loss: 0.2895 - val_acc: 0.9114\n",
            "Epoch 17/30\n",
            "7352/7352 [==============================] - 68s 9ms/step - loss: 0.1550 - acc: 0.9455 - val_loss: 0.2922 - val_acc: 0.9175\n",
            "Epoch 18/30\n",
            "7352/7352 [==============================] - 67s 9ms/step - loss: 0.1115 - acc: 0.9529 - val_loss: 0.2962 - val_acc: 0.9145\n",
            "Epoch 19/30\n",
            "7352/7352 [==============================] - 68s 9ms/step - loss: 0.1249 - acc: 0.9532 - val_loss: 0.2872 - val_acc: 0.9155\n",
            "Epoch 20/30\n",
            "7352/7352 [==============================] - 67s 9ms/step - loss: 0.1083 - acc: 0.9555 - val_loss: 0.2752 - val_acc: 0.9158\n",
            "Epoch 21/30\n",
            "7352/7352 [==============================] - 67s 9ms/step - loss: 0.1668 - acc: 0.9429 - val_loss: 0.3213 - val_acc: 0.8873\n",
            "Epoch 22/30\n",
            "7352/7352 [==============================] - 66s 9ms/step - loss: 0.1175 - acc: 0.9547 - val_loss: 0.3037 - val_acc: 0.9080\n",
            "Epoch 23/30\n",
            "7352/7352 [==============================] - 100s 14ms/step - loss: 0.1130 - acc: 0.9554 - val_loss: 0.2845 - val_acc: 0.9084\n",
            "Epoch 24/30\n",
            "7352/7352 [==============================] - 139s 19ms/step - loss: 0.1037 - acc: 0.9576 - val_loss: 0.2526 - val_acc: 0.9223\n",
            "Epoch 25/30\n",
            "7352/7352 [==============================] - 66s 9ms/step - loss: 0.1047 - acc: 0.9581 - val_loss: 0.2509 - val_acc: 0.9186\n",
            "Epoch 26/30\n",
            "7352/7352 [==============================] - 67s 9ms/step - loss: 0.1012 - acc: 0.9588 - val_loss: 0.2464 - val_acc: 0.9223\n",
            "Epoch 27/30\n",
            "7352/7352 [==============================] - 69s 9ms/step - loss: 0.1021 - acc: 0.9574 - val_loss: 0.2211 - val_acc: 0.9240\n",
            "Epoch 28/30\n",
            "7352/7352 [==============================] - 67s 9ms/step - loss: 0.1506 - acc: 0.9479 - val_loss: 0.2350 - val_acc: 0.9274\n",
            "Epoch 29/30\n",
            "7352/7352 [==============================] - 95s 13ms/step - loss: 0.1134 - acc: 0.9559 - val_loss: 0.2411 - val_acc: 0.9128\n",
            "Epoch 30/30\n",
            "7352/7352 [==============================] - 127s 17ms/step - loss: 0.1181 - acc: 0.9533 - val_loss: 0.2815 - val_acc: 0.9145\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcb58337978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "metadata": {
        "id": "55xymedXAQx3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D5iAfxEh2lsQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Comparsions"
      ]
    },
    {
      "metadata": {
        "id": "B7QGg8xm2oZB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "res=pd.DataFrame( {'Batchsize':[16,128,8,16],\n",
        "               'Dropout':['-',0.7,0.7,0.7],\n",
        "               'optimizer':['rmsprop','adam','adam','adam'],\n",
        "               'units':[32,32,32,64],\n",
        "               'validation_data':[('x_test', 'y_test'),'0.2*traindata' ,'0.2*traindata' , ('x_test', 'y_test')] ,\n",
        "               'loss': [0.1685,0.9776,0.3341,0.1181] , \n",
        "               'acc': [0.9483,0.5531,0.9022 ,0.9533] , \n",
        "               'val_loss' : [0.5401,1.0182,0.5777,0.2815],\n",
        "              'val_acc': [0.8785,0.5744,0.8708,0.9415]} , index=['LSTM-rmsprop','LSTM-adam-batchsize128','LSTM-adam-batchsize8','2LSTMcells-hiden32-hyperparameter'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XyadLvOM_hvH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "517abca7-ac50-41de-f45e-ddeff37b70aa"
      },
      "cell_type": "code",
      "source": [
        "from prettytable import PrettyTable\n",
        "    \n",
        "x = PrettyTable()\n",
        "\n",
        "x.field_names = [\"ALGO\" , \"Batchsize\" ,'Dropout','optimizer','units','Train_loss','Train_acc','Test_loss','Test_acc']\n",
        "\n",
        "x.add_row([ \"LSTM-rmsprop \" ,16,'-' ,'rmsprop',32,0.1685,0.9483,0.5401,0.8785 ]) \n",
        "x.add_row([ \"LSTM-adam-batchsize128 \" ,128,0.7 ,'adam',32,0.9776,0.5531,1.0182,0.5744 ]) \n",
        "x.add_row([ \"LSTM-adam-batchsize8 \" ,8,0.7 ,'adam',32,0.3341,0.9022,0.5777,0.8708]) \n",
        "x.add_row([ \"2LSTMcells-hiden32-hyperparameter \" ,16,0.7,'adam',64,0.1181,0.9533,0.2815,0.9415 ]) \n",
        "\n",
        "#x.add_row([ \"Logistic Regression \" , {'alpha' : 0.1 }  , 0.3243 , 0.4219 ])\n",
        "#x.add#_row([ \"Linear SVM\" , {'alpha' :0.00001} , 0.4858, 0.5419 ])\n",
        "#x.add_row([ \"XGBost\" , {'max_depth': 4, 'min_child_weight': 2, 'reg_alpha': 0.001} , 0.3540, 0.3593 ])\n",
        "\n",
        "print(x)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------------------------+-----------+---------+-----------+-------+------------+-----------+-----------+----------+\n",
            "|                ALGO                | Batchsize | Dropout | optimizer | units | Train_loss | Train_acc | Test_loss | Test_acc |\n",
            "+------------------------------------+-----------+---------+-----------+-------+------------+-----------+-----------+----------+\n",
            "|           LSTM-rmsprop             |     16    |    -    |  rmsprop  |   32  |   0.1685   |   0.9483  |   0.5401  |  0.8785  |\n",
            "|      LSTM-adam-batchsize128        |    128    |   0.7   |    adam   |   32  |   0.9776   |   0.5531  |   1.0182  |  0.5744  |\n",
            "|       LSTM-adam-batchsize8         |     8     |   0.7   |    adam   |   32  |   0.3341   |   0.9022  |   0.5777  |  0.8708  |\n",
            "| 2LSTMcells-hiden32-hyperparameter  |     16    |   0.7   |    adam   |   64  |   0.1181   |   0.9533  |   0.2815  |  0.9415  |\n",
            "+------------------------------------+-----------+---------+-----------+-------+------------+-----------+-----------+----------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pcg7NNON4m0o",
        "colab_type": "code",
        "outputId": "6f839d05-965d-461a-d554-d3f858c53abf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 878
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "a=res[['acc','loss','val_acc','val_loss']]\n",
        "a.plot(kind='bar')\n",
        "plt.rcParams[\"figure.figsize\"] = (5,5)\n",
        "plt.title(\"Plot of LSTM MOdels \",fontsize=20)\n",
        "plt.xlabel(\"Models\",fontsize=20)\n",
        "plt.ylabel(\"Loss & Accuracy\",fontsize=20)\n",
        "plt.xticks(fontsize=14) #rotation=90)\n",
        "plt.yticks(fontsize=14)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABJsAAANJCAYAAACrpj23AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "So3-Q3Oq_5dE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}